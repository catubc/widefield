{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import gridspec\n",
    "import parmap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import parmap\n",
    "import shutil\n",
    "import cv2\n",
    "import scipy.io as sio\n",
    "from Specgram import Specgram\n",
    "import csv\n",
    "\n",
    "import glob2\n",
    "\n",
    "from numba import jit\n",
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "\n",
    "colors = [\n",
    "'black','blue','red','green', 'cyan','orange','brown','slategrey','darkviolet','darkmagenta',\n",
    "'lawngreen','dodgerblue','crimson','orchid','slateblue',\n",
    "'darkgreen','darkorange','indianred','darkviolet','deepskyblue','greenyellow',\n",
    "'peru','cadetblue','forestgreen','slategrey','lightsteelblue','rebeccapurple',\n",
    "'darkmagenta','yellow','hotpink']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mask(fname):\n",
    "    # load mask for dataset \n",
    "    fname_mask = fname\n",
    "    for k in range(3):\n",
    "        fname_mask = os.path.split(fname_mask)[0]\n",
    "    fname_mask+=\"/genericmask.txt\"\n",
    "    print (fname_mask)\n",
    "\n",
    "    mask = np.int32(np.loadtxt(fname_mask))\n",
    "    raw = np.ones((128,128),'float32')\n",
    "    for k in range(mask.shape[0]):\n",
    "        raw[mask[k][0], mask[k][1]]=np.nan\n",
    "#     plt.imshow(raw)\n",
    "#     plt.show()\n",
    "\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/cat/4TBSSD/yuki/IA2/genericmask.txt\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/media/cat/4TBSSD/yuki/IA2/genericmask.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-28f34c716bcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_aligned.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmask_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-2f5a9e5e6fe2>\u001b[0m in \u001b[0;36mload_mask\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfname_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    533\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    534\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: /media/cat/4TBSSD/yuki/IA2/genericmask.txt not found."
     ]
    }
   ],
   "source": [
    "# Initialize filenames and masks\n",
    "#root_dir = '/media/cat/4TBSSD/yuki/IA2/tif_files/IA2pm_Feb17_30Hz/'\n",
    "root_dir = '/media/cat/4TBSSD/yuki/IA2/tif_files/IA2pm_Apr22_Week2_30Hz/'\n",
    "fname = root_dir + os.path.split(root_dir[:-1])[1]+'_aligned.npy'\n",
    "\n",
    "mask_original = load_mask(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask_original)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2.73333333    8.           16.66666667   56.26666667  125.46666667\n",
      "  218.86666667  275.66666667  289.4         307.8         340.6\n",
      "  416.6         435.66666667  452.2         555.06666667  782.\n",
      "  833.66666667  878.33333333  910.8         983.66666667 1074.8\n",
      " 1100.33333333 1127.2        1164.4       ]\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/media/cat/4TBSSD/yuki/IA2/tif_files/IA2pm_Apr22_Week2_30Hz/triggers/starts_grooming.npy')\n",
    "print (data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisnihed loading arrays\n",
      "(390, 128, 128)\n",
      "8\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Fig 2A triggered motiffs;\n",
    "data = np.load(fname)\n",
    "labels = np.load(root_dir + '/triggers/labels.npy', allow_pickle=True)\n",
    "\n",
    "window = np.load(root_dir + '/triggers/window.npy')\n",
    "\n",
    "# LOAD ALL THE TRIGGERS FOR THE FEATURE BASED DATA;\n",
    "starts_arrays = np.load(root_dir + '/triggers/starts_arrays.npy', allow_pickle=True)\n",
    "\n",
    "# ADD GROOMING EVENTS\n",
    "starts_grooming = np.float32(np.load(root_dir+'/triggers/starts_grooming.npy', allow_pickle=True))\n",
    "starts_arrays = np.concatenate((starts_arrays,starts_grooming))\n",
    "starts_arrays[7] = starts_grooming\n",
    "\n",
    "# add the last label to the stack\n",
    "labels = np.append(labels,'grooming')\n",
    "\n",
    "# METRICS FOR COMPUTING EVENT TRIGGERED AVERAGES\n",
    "bin_ave = 15\n",
    "sample_rate = 30\n",
    "n_cols = 12    \n",
    "\n",
    "# save averages for each feature triggered set\n",
    "stacks_ave_arrays = []\n",
    "fname_dff = root_dir + 'dff_data.npy'\n",
    "start = -3 # seconds start from trigger point\n",
    "end = +10\n",
    "\n",
    "if os.path.exists(fname_dff)==False:\n",
    "    # select features\n",
    "    features = np.arange(len(labels))\n",
    "    ctr=0\n",
    "    fig=plt.figure(figsize=(20,20))\n",
    "    for feature in features:\n",
    "        print (\"feature: \", feature)\n",
    "        #feature = 1\n",
    "        starts_frametime = np.int32(starts_arrays[feature]*sample_rate)\n",
    "        #print (labels[feature], \", starts_frametime: \", starts_frametime)\n",
    "\n",
    "\n",
    "        # get neural activity stack with extra 3 seconds at the beginning of each bit\n",
    "        stack = []\n",
    "        dff_window = 3\n",
    "        for k in range(starts_frametime.shape[0]):\n",
    "            try:\n",
    "                temp = data[starts_frametime[k]+int(start*sample_rate)-dff_window*sample_rate: starts_frametime[k]+int(end*sample_rate)]\n",
    "\n",
    "                # only add events that are correct width; correct for data falling off edges;\n",
    "                if temp.shape[0] == int(end-start)*sample_rate+dff_window*sample_rate:\n",
    "                    stack.append(temp)\n",
    "            except:\n",
    "                pass\n",
    "        stack = np.array(stack)\n",
    "\n",
    "        # P\n",
    "        print (\"LOaded stack: \", stack.shape)\n",
    "\n",
    "        # compute DF/F\n",
    "\n",
    "        if True:\n",
    "            stack_dff = np.zeros((stack.shape[0], int(end-start)*sample_rate, stack.shape[2], stack.shape[3]),'float32')\n",
    "            print (\"stack ave: \", stack_dff.shape)\n",
    "            for j in range(stack.shape[0]):\n",
    "                for k in range(128):\n",
    "                    for p in range(128):\n",
    "                        F0 = stack[j,:dff_window*sample_rate,k,p].mean(0)\n",
    "                        stack_dff[j,:,k,p] = (stack[j,dff_window*sample_rate:,k,p]-F0)/F0\n",
    "\n",
    "            #stack_ave = stack_dff.mean(0)\n",
    "            stack_ave = np.median(stack_dff,axis=0)\n",
    "        else:\n",
    "            # compute DF/F\n",
    "            stack_ave = stack.mean(0)\n",
    "            for k in range(128):\n",
    "                for p in range(128):\n",
    "                    stack_ave[:,k,p] = stack_ave[:,k,p]-stack_ave[:stack_ave.shape[0]//2,k,p].mean(0)           \n",
    "\n",
    "        stacks_ave_arrays.append(stack_ave)\n",
    "\n",
    "\n",
    "        vmax = np.max(np.abs(stack_ave))\n",
    "        for k in range(0,180,bin_ave):\n",
    "            #print (len(features),n_cols,k//bin_ave+1+n_cols*ctr)\n",
    "            ax=plt.subplot(len(features),n_cols,k//bin_ave+1+n_cols*ctr)\n",
    "            if k==0:\n",
    "                plt.ylabel(labels[feature]+\": \"+str(starts_frametime.shape[0]),fontsize=10)\n",
    "            plt.imshow(stack_ave[k:k+bin_ave].mean(0)*mask_original, vmin=-vmax, vmax=vmax)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            #plt.title(str(round((k-stack.shape[1]/2.)/30.,2)))\n",
    "        ctr+=1\n",
    "\n",
    "    #plt.savefig('/media/cat/4TBSSD/yuki/IA2/tif_files/IA2pm_Apr22_Week2_30Hz/triggers/triggered.png',\n",
    "    #           dpi=200)\n",
    "    plt.show()\n",
    "    \n",
    "    np.save(fname_dff, stacks_ave_arrays)\n",
    "else:\n",
    "    stacks_ave_arrays = np.load(fname_dff)\n",
    "\n",
    "    print (\"Fisnihed loading arrays\")\n",
    "\n",
    "print (stacks_ave_arrays[0].shape)\n",
    "print (len(stacks_ave_arrays))\n",
    "print (\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 180, 128, 128)\n",
      "stack 3d:  (84, 180, 16384)\n",
      "stack in:  (15120, 16384)\n",
      "(65, 180, 128, 128)\n",
      "stack 3d:  (65, 180, 16384)\n",
      "stack in:  (11700, 16384)\n",
      "stack_out:  (26820, 16384)\n",
      "PCS out:  (26820, 100)\n",
      "DONE PCA Block\n",
      "DONE PROCESSING PCA \n"
     ]
    }
   ],
   "source": [
    "# Visualize PCA space \n",
    "\n",
    "# compute PCA on all trials all times \n",
    "\n",
    "# which features to use to make PCA matrix; too many will take too long\n",
    "features = [0,5]\n",
    "\n",
    "if True:\n",
    "    stack_out = []\n",
    "    for feature in features:\n",
    "        starts_frametime = np.int32(starts_arrays[feature]*sample_rate)\n",
    "\n",
    "        stack = []\n",
    "        for k in range(starts_frametime.shape[0]):\n",
    "            try:\n",
    "                temp = data[starts_frametime[k]-int(window*sample_rate): starts_frametime[k]+int(window*sample_rate)]\n",
    "                if temp.shape[0] == int(window*sample_rate*2):\n",
    "                    stack.append(temp)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        stack = np.array(stack)\n",
    "        print (stack.shape)\n",
    "        stack_3D= stack.reshape(stack.shape[0],stack.shape[1],-1)\n",
    "        print (\"stack 3d: \", stack_3D.shape)\n",
    "\n",
    "        stack_in = []\n",
    "        for k in range(stack_3D.shape[0]):\n",
    "            stack_in.extend(stack_3D[k])\n",
    "        stack_in=np.vstack(stack_in)\n",
    "        print (\"stack in: \", stack_in.shape)\n",
    "        stack_out.append(stack_in)\n",
    "\n",
    "    stack_out = np.vstack(stack_out)\n",
    "    print (\"stack_out: \", stack_out.shape)\n",
    "    # run PCA on all data\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca = PCA(n_components=100)\n",
    "    pca_model = pca.fit(stack_out)\n",
    "\n",
    "    PCS = pca_model.transform(stack_out)\n",
    "    print (\"PCS out: \", PCS.shape)\n",
    "    print (\"DONE PCA Block\")\n",
    "\n",
    "# compute PCA only on averages\n",
    "else:\n",
    "    \n",
    "    stack_out = []\n",
    "    for feature in features:\n",
    "        starts_frametime = np.int32(starts_arrays[feature]*sample_rate)\n",
    "\n",
    "        stack = []\n",
    "        for k in range(starts_frametime.shape[0]):\n",
    "            try:\n",
    "                temp = data[starts_frametime[k]-int(window*sample_rate): starts_frametime[k]+int(window*sample_rate)]\n",
    "                if temp.shape[0] == int(window*sample_rate*2):\n",
    "                    stack.append(temp)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        stack = np.array(stack)\n",
    "        print (stack.shape)\n",
    "        stack_3D= stack.reshape(stack.shape[0],stack.shape[1],-1)\n",
    "        print (\"stack 3d: \", stack_3D.shape)\n",
    "\n",
    "        stack_in=stack_3D.mean(0)\n",
    "        print (\"stack in: \", stack_in.shape)\n",
    "        stack_out.append(stack_in)\n",
    "\n",
    "    stack_out = np.vstack(stack_out)\n",
    "    print (\"stack_out: \", stack_out.shape)\n",
    "    # run PCA on all data\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca = PCA(n_components=100)\n",
    "    pca_model = pca.fit(stack_out)\n",
    "\n",
    "    PCS = pca_model.transform(stack_out)\n",
    "    print (\"PCS out: \", PCS.shape)\n",
    "    print (\"DONE PCA Block\")\n",
    "    \n",
    "print (\"DONE PROCESSING PCA \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "plt.scatter(PCS[:,0], PCS[:,1],alpha=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot explained variance\n",
    "fig=plt.figure()\n",
    "expl_variance = pca_model.explained_variance_[:10]\n",
    "plt.scatter(np.arange(expl_variance.shape[0]), expl_variance/expl_variance.sum(0),s=1000)\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel(\"Explained Variance\",fontsize=40)\n",
    "plt.xlabel(\"Principal component\",fontsize=40)\n",
    "plt.tick_params(axis = 'both', which = 'major', labelsize = 40)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.load('/media/cat/4TBSSD/yuki/IA2/tif_files/IA2pm_Apr22_Week2_30Hz/IA2pm_Apr22_Week2_30Hz_aligned_maskwarp.npy')\n",
    "if True:\n",
    "    idx = np.where(mask!=0)\n",
    "    mask[idx]=1\n",
    "\n",
    "    idx = np.where(mask==0)\n",
    "    mask[idx]=np.nan\n",
    "#mask[~idx]=1\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 16384)\n",
      "[9.75446591e+12 8.48452550e+11 1.89249936e+11 8.85156429e+10\n",
      " 7.59601754e+10 5.69121674e+10 2.88994562e+10 1.97761603e+10\n",
      " 1.49791553e+10 1.03343404e+10]\n"
     ]
    }
   ],
   "source": [
    "print (pca_model.components_.shape)\n",
    "print (pca_model.singular_values_[:10]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16384, 100)\n"
     ]
    }
   ],
   "source": [
    "# load the components\n",
    "loadings = pca_model.components_.T #* np.sqrt(pca_model.explained_variance_)\n",
    "print (loadings.shape)\n",
    "\n",
    "# \n",
    "plt.title(\"First 20 Loadings\",fontsize=40)\n",
    "for k in range(9):\n",
    "    ax = plt.subplot(3,3,k+1)\n",
    "    plt.title(\"Comp: \"+str(k))\n",
    "    \n",
    "    temp = loadings[:,k].reshape(128,128)\n",
    "    #vmax = np.max(np.abs(temp))\n",
    "    plt.imshow(temp*mask_original)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "plt.suptitle(\"PC component * explained variance \")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack shape:  (84, 180, 128, 128)\n",
      "stack 3d:  (84, 180, 16384)\n",
      "PCS feature:  (180, 100)\n",
      "pcs 3D:  (180, 100)\n",
      "window:  3.0\n",
      " starts_secondary_features:  [1.33333333e-01 1.90666667e+01 3.03333333e+01 3.99333333e+01\n",
      " 7.80000000e+01 9.62666667e+01 1.05133333e+02 1.13600000e+02\n",
      " 1.23133333e+02 1.38933333e+02]\n",
      " starts_primary_features:  [1.33333333e-01 1.63333333e+01 4.00000000e+01 5.08000000e+01\n",
      " 6.90000000e+01 9.60666667e+01 1.05133333e+02 1.23066667e+02\n",
      " 1.38466667e+02 1.47066667e+02]\n",
      "window:  3.0\n",
      " starts_secondary_features:  [  0.73333333  16.6         37.66666667  65.73333333  69.\n",
      "  95.8        123.         138.46666667 173.13333333 187.2       ]\n",
      " starts_primary_features:  [1.33333333e-01 1.63333333e+01 4.00000000e+01 5.08000000e+01\n",
      " 6.90000000e+01 9.60666667e+01 1.05133333e+02 1.23066667e+02\n",
      " 1.38466667e+02 1.47066667e+02]\n",
      "window:  3.0\n",
      " starts_secondary_features:  [ 0.13333333 19.6        30.6        40.         52.26666667 63.2\n",
      " 68.73333333 78.06666667 84.66666667 96.2       ]\n",
      " starts_primary_features:  [1.33333333e-01 1.63333333e+01 4.00000000e+01 5.08000000e+01\n",
      " 6.90000000e+01 9.60666667e+01 1.05133333e+02 1.23066667e+02\n",
      " 1.38466667e+02 1.47066667e+02]\n",
      "window:  3.0\n",
      " starts_secondary_features:  [  0.86666667  16.33333333  40.2         50.73333333  69.13333333\n",
      "  77.93333333  95.93333333 105.2        113.66666667 123.06666667]\n",
      " starts_primary_features:  [1.33333333e-01 1.63333333e+01 4.00000000e+01 5.08000000e+01\n",
      " 6.90000000e+01 9.60666667e+01 1.05133333e+02 1.23066667e+02\n",
      " 1.38466667e+02 1.47066667e+02]\n",
      "window:  3.0\n",
      " starts_secondary_features:  [2.00000000e-01 1.63333333e+01 4.79333333e+01 6.23333333e+01\n",
      " 9.42000000e+01 1.35666667e+02 1.68933333e+02 2.05000000e+02\n",
      " 2.16666667e+02 2.44400000e+02]\n",
      " starts_primary_features:  [1.33333333e-01 1.63333333e+01 4.00000000e+01 5.08000000e+01\n",
      " 6.90000000e+01 9.60666667e+01 1.05133333e+02 1.23066667e+02\n",
      " 1.38466667e+02 1.47066667e+02]\n",
      "window:  3.0\n",
      " starts_secondary_features:  [  1.53333333   7.46666667  16.6         51.46666667  69.8\n",
      "  96.53333333 105.6        123.46666667 139.53333333 148.46666667]\n",
      " starts_primary_features:  [1.33333333e-01 1.63333333e+01 4.00000000e+01 5.08000000e+01\n",
      " 6.90000000e+01 9.60666667e+01 1.05133333e+02 1.23066667e+02\n",
      " 1.38466667e+02 1.47066667e+02]\n",
      "window:  3.0\n",
      " starts_secondary_features:  [  2.7333333   8.         16.666666   56.266666  125.46667   218.86667\n",
      " 275.66666   289.4       307.8       340.6      ]\n",
      " starts_primary_features:  [1.33333333e-01 1.63333333e+01 4.00000000e+01 5.08000000e+01\n",
      " 6.90000000e+01 9.60666667e+01 1.05133333e+02 1.23066667e+02\n",
      " 1.38466667e+02 1.47066667e+02]\n"
     ]
    }
   ],
   "source": [
    "# FIG 2C:  Dynamics PC1 vs PC2\n",
    "\n",
    "ctr=0\n",
    "clrs = ['green','blue','brown','black','red','cyan','magenta','yellow']\n",
    "\n",
    "features = np.arange(8)\n",
    "stack_out = []\n",
    "#for feature in features:\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "#for feature in features:\n",
    "pcX = 0\n",
    "pcY = 1\n",
    "for feature in [0]:\n",
    "#for feature in features:    # compute start times of selected feature in seconds\n",
    "    starts_frametime = np.int32(starts_arrays[feature]*sample_rate)\n",
    "\n",
    "    # grab raw imaging frames \n",
    "    stack = []\n",
    "    for k in range(starts_frametime.shape[0]):\n",
    "        try:\n",
    "            temp = data[starts_frametime[k]-int(window*sample_rate): starts_frametime[k]+int(window*sample_rate)]\n",
    "            if temp.shape[0] == int(window*sample_rate*2):\n",
    "                stack.append(temp)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    stack = np.array(stack)\n",
    "    print (\"stack shape: \", stack.shape)\n",
    "    \n",
    "    # flatten last 2 dimensions\n",
    "    stack_3D= stack.reshape(stack.shape[0],stack.shape[1],-1)\n",
    "    print (\"stack 3d: \", stack_3D.shape)\n",
    "\n",
    "    # run PCA on single trials\n",
    "    if False:\n",
    "        stack_in = []\n",
    "        for k in range(stack_3D.shape[0]):\n",
    "            stack_in.extend(stack_3D[k])\n",
    "        stack_in=np.vstack(stack_in)\n",
    "        \n",
    "        # convert data to PCs\n",
    "        PCS_feature = pca_model.transform(stack_in)\n",
    "        print (\"PCS feature: \", PCS_feature.shape)\n",
    "        pcs_3d = []\n",
    "        for p in range(0,PCS_feature.shape[0], stack_3D.shape[1]):\n",
    "            pcs_3d.append(PCS_feature[p:p+stack_3D.shape[1]])\n",
    "        pcs_3d = np.array(pcs_3d)\n",
    "        print (pcs_3d.shape)\n",
    "                \n",
    "        ave = []\n",
    "        for k in range(pcs_3d.shape[0]):\n",
    "            ave.append(pcs_3d[k,:])\n",
    "\n",
    "        ave= np.array(ave)\n",
    "        colors = plt.cm.viridis(np.linspace(0,1,ave.shape[1]))\n",
    "\n",
    "        ave_ave = ave.mean(0)\n",
    "        plt.scatter(ave_ave[0,pcX], ave_ave[0,pcY], s=500, alpha=1, c=clrs[ctr])\n",
    "        if False:\n",
    "            for k in range(0,ave_ave.shape[0]-1,1):\n",
    "                plt.plot(ave_ave[k:k+2,pcX], ave_ave[k:k+2,pcY], linewidth=5, \n",
    "                         color=cmap(k))\n",
    "        else:\n",
    "            for k in range(0,ave_ave.shape[0]-1,1):\n",
    "                \n",
    "                if k==0:\n",
    "                    plt.plot(ave_ave[k:k+2,pcX], ave_ave[k:k+2,pcY], linewidth=5, \n",
    "                        color=clrs[feature],\n",
    "                        alpha=1,label=labels[feature])\n",
    "                else:\n",
    "                    plt.plot(ave_ave[k:k+2,pcX], ave_ave[k:k+2,pcY], linewidth=5, \n",
    "                        color=clrs[feature],\n",
    "                        alpha=1-(k/ave_ave.shape[0]*0.35+.35))\n",
    "\n",
    "        #plt.text(ave_ave[90,0]+1000, ave_ave[90,1],'t=0sec')\n",
    "        plt.scatter(ave_ave[90,pcX], ave_ave[90,pcY], s=500, hatch='++', alpha=1, c=clrs[ctr])\n",
    "        plt.scatter(ave_ave[179,pcX], ave_ave[179,pcY], s=500, hatch='///', alpha=1, c=clrs[ctr])\n",
    "\n",
    "        plt.legend(fontsize=20)\n",
    "    # run PCA on average only\n",
    "    else:\n",
    "        \n",
    "        #ax=plt.subplot(2,4,feature+1)\n",
    "        ax=plt.subplot(1,1,1)\n",
    "        \n",
    "        # RUN PCA ON THE AVERAGE OF ALL TRACES IN A STACK\n",
    "        stack_in=stack_3D.mean(0)\n",
    "        PCS_feature = pca_model.transform(stack_in)\n",
    "        print (\"PCS feature: \", PCS_feature.shape)\n",
    "        pcs_3d = []\n",
    "        for p in range(0,PCS_feature.shape[0], stack_3D.shape[1]):\n",
    "            pcs_3d.append(PCS_feature[p:p+stack_3D.shape[1]])\n",
    "        pcs_3d = np.array(pcs_3d).squeeze()\n",
    "        print (\"pcs 3D: \", pcs_3d.shape)\n",
    "        \n",
    "        \n",
    "        # *****************************************\n",
    "        # PLOT STEP\n",
    "        #colors = plt.cm.viridis(np.linspace(0,1,pcs_3d.shape[0]))\n",
    "        cmap = cm.get_cmap('viridis',pcs_3d.shape[0])\n",
    "\n",
    "        plt.scatter(pcs_3d[0,pcX], pcs_3d[0,pcY], s=250, alpha=1, c=clrs[ctr])\n",
    "        # plot bits at a time to use alpha\n",
    "        if False: \n",
    "            for k in range(0,pcs_3d.shape[0]-1,1):\n",
    "                #plt.plot(pcs_3d[k:k+2,0], pcs_3d[k:k+2,1], linewidth=5, color=cmap(k))\n",
    "                plt.plot(pcs_3d[k:k+2,pcX], pcs_3d[k:k+2,pcY], linewidth=5, color=clrs[ctr],alpha=(180-k)/200)\n",
    "        else:\n",
    "            plt.plot(pcs_3d[:,pcX], pcs_3d[:,pcY], linewidth=6, color=clrs[ctr],alpha=.7)\n",
    "            \n",
    "        # plot the locations of other features in the timeseries\n",
    "        for j in features:\n",
    "            \n",
    "            # skip self triggered location\n",
    "            if j==feature:\n",
    "                continue\n",
    "            starts_primary_features = starts_arrays[feature]\n",
    "            \n",
    "            # find other features starts in the same window as the main features selected:\n",
    "            starts_secondary_features = starts_arrays[j]\n",
    "            \n",
    "            # loop over each starts_frametime\n",
    "            # loop over each starts_frametime\n",
    "            print (\"window: \", window)\n",
    "            print (\" starts_secondary_features: \", starts_secondary_features[:10])\n",
    "            print (\" starts_primary_features: \", starts_primary_features[:10])\n",
    "            \n",
    "            for p in range(starts_primary_features.shape[0]):\n",
    "                idx = np.where(np.logical_and(starts_secondary_features>=starts_primary_features[p]-window,\n",
    "                                              starts_secondary_features<=starts_primary_features[p]+window))\n",
    "                \n",
    "                secondary_starts_seconds = starts_secondary_features[idx]-starts_primary_features[p]+window\n",
    "                #print (\"feature: \", labels[j], \" secondary starts seconds; \", secondary_starts_seconds)\n",
    "\n",
    "                secondary_starts_frames = np.int32(secondary_starts_seconds*sample_rate)\n",
    "                idx = np.where(np.logical_and(secondary_starts_frames>=0, secondary_starts_frames<180))[0]\n",
    "                secondary_starts_frames=secondary_starts_frames[idx]\n",
    "\n",
    "                #print (\"feature: \", labels[j], \" secondary starts frames; \", secondary_starts_frames)\n",
    "                for k in secondary_starts_frames:\n",
    "                    plt.scatter(pcs_3d[k,pcX]+np.random.rand()*1000-500, \n",
    "                                pcs_3d[k,pcY]+np.random.rand()*1000-500, \n",
    "                                s=10, \n",
    "                                color=clrs[j],\n",
    "                               alpha=.8)\n",
    "        \n",
    "        # plot star\n",
    "        plt.scatter(pcs_3d[0,pcX], pcs_3d[0,pcY], s=250, c=clrs[ctr], edgecolors='black',alpha=.5)\n",
    "        plt.scatter(pcs_3d[pcs_3d.shape[0]//2,pcX], pcs_3d[pcs_3d.shape[0]//2,pcY], s=250, c=clrs[ctr], edgecolors='black',alpha=.5)\n",
    "        plt.scatter(pcs_3d[-1,pcX], pcs_3d[-1,pcY], s=250, c=clrs[ctr], edgecolors='black', alpha=.5)\n",
    "        plt.text(pcs_3d[0,pcX]+1000, pcs_3d[0,pcY],'t=-3sec',fontsize=20)\n",
    "        plt.text(pcs_3d[pcs_3d.shape[0]//2,pcX]+1000, pcs_3d[pcs_3d.shape[0]//2,pcY],'t=0sec',fontsize=20)\n",
    "        plt.text(pcs_3d[-1,pcX]+1000, pcs_3d[-1,pcY],'t=+3sec',fontsize=20)\n",
    "\n",
    "        \n",
    "        # Format plot\n",
    "        plt.xlabel(\"PC\"+str(pcX+1),fontsize=20)\n",
    "        plt.ylabel(\"PC\"+str(pcY+1),fontsize=20)\n",
    "        #plt.xlim(-10000,12000)\n",
    "        #plt.ylim(-10000,12000)\n",
    "\n",
    "        #plt.plot([0,0],[-10000,12000],'r--', c='black')\n",
    "        #plt.plot([-10000,12000],[0,0],'r--',c='black')\n",
    "   \n",
    "    plt.title(labels[feature])\n",
    "    \n",
    "    ctr+=1\n",
    "if False:\n",
    "    plt.savefig('/home/cat/tcs.png',dpi=100)\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "########### VISUALIZE REGISTERED REGIONS ###########\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/cat/4TBSSD/yuki/IA2/tif_files/IA2pm_Apr22_Week2_30Hz/IA2pm_Apr22_Week2_30Hz_aligned_maskwarp.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bcefb5c16726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/media/cat/4TBSSD/yuki/IA2/tif_files/IA2pm_Apr22_Week2_30Hz/IA2pm_Apr22_Week2_30Hz_aligned_maskwarp.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/cat/4TBSSD/yuki/IA2/tif_files/IA2pm_Apr22_Week2_30Hz/IA2pm_Apr22_Week2_30Hz_aligned_maskwarp.npy'"
     ]
    }
   ],
   "source": [
    "mask = np.load('/media/cat/4TBSSD/yuki/IA2/tif_files/IA2pm_Apr22_Week2_30Hz/IA2pm_Apr22_Week2_30Hz_aligned_maskwarp.npy')\n",
    "if True:\n",
    "    idx = np.where(mask!=0)\n",
    "    mask[idx]=1\n",
    "\n",
    "    idx = np.where(mask==0)\n",
    "    mask[idx]=np.nan\n",
    "#mask[~idx]=1\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/cat/dorsalMaps_name.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ddf5179b7e0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load REGISTERED MAP IN 679 x 586 resolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdorsalMaps_unique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/cat/dorsalMaps_name.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdorsalMaps_unique\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdorsalMaps_unique\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/cat/dorsalMaps_name.npy'"
     ]
    }
   ],
   "source": [
    "# load REGISTERED MAP IN 679 x 586 resolution\n",
    "\n",
    "dorsalMaps_unique = np.load('/home/cat/dorsalMaps_name.npy')\n",
    "print (dorsalMaps_unique.shape)\n",
    "print (np.unique(dorsalMaps_unique))\n",
    "plt.imshow(dorsalMaps_unique)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   2.   3.   4.   5.   7.   8.   9.  10.  11.  12.  13.  14.\n",
      "  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n",
      "  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.\n",
      "  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  55.  56.  57.\n",
      "  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.  71.\n",
      "  72.  73.  74.  75.  76.  78.  79.  80.  81.  83.  84.  85.  86.  88.\n",
      "  90.  91.  92.  93.  94.  95.  96.  97.  99. 100. 101. 102. 103. 104.\n",
      " 106. 107. 108. 113. 114. 115. 116. 117. 119. 120. 121. 123. 124. 125.\n",
      " 127. 128. 129. 130. 132. 134. 135. 136. 137. 138. 139. 140. 141. 142.\n",
      " 143. 144. 146. 147. 148. 149. 150. 151. 152. 153. 154. 155. 156. 157.\n",
      " 158. 159. 160. 161. 162. 163. 164. 165. 166. 167. 168. 170. 171. 172.\n",
      " 173. 174. 175. 176. 177. 178. 179. 180. 182. 183. 184. 185. 186. 187.\n",
      " 188. 189. 190. 191. 194. 195. 196. 197. 198. 199. 200. 201. 203. 204.\n",
      " 206. 208. 210. 214. 215. 216. 217. 218. 220. 221. 222. 223. 225. 228.\n",
      " 229. 230. 232. 234. 235. 236. 237. 238. 240. 241. 242. 243. 245. 246.\n",
      " 247. 248. 249. 250. 251. 252. 253. 254. 255. 256. 257. 258. 259. 260.\n",
      " 261. 263. 265. 266. 267. 268. 269. 270. 271. 272. 275. 278. 280. 282.\n",
      " 290. 295. 297. 300. 301. 302. 303. 313. 325. 328. 333. 337. 361. 362.\n",
      " 387. 435. 455. 596. 621. 623. 647. 651. 652. 653.]\n",
      "[  0.   8.  15.  21.  29.  36.  43.  50.  57.  64.  71.  78.  92. 100.\n",
      " 107. 114. 121. 129. 136. 143. 150. 157. 164. 171. 178. 186. 198. 217.\n",
      " 249. 255. 261. 268. 275. 282. 295. 300. 301. 651. 653.]\n"
     ]
    }
   ],
   "source": [
    "# load REGISTERED MAP IN 128 x 128 resolution\n",
    "# NOTE: This map contains average errors that need to be removed:\n",
    "# so replace the values which are not in the dorsalMap list above with most frequent nearby values;\n",
    "\n",
    "# fix mean maskwarp values\n",
    "maskwarp = np.load('/home/cat/maskwarp.npy')\n",
    "ax = plt.subplot(1,2,1)\n",
    "plt.imshow(maskwarp)\n",
    "print (np.unique(maskwarp))\n",
    "maskwarp_unique_ids = np.unique(maskwarp)\n",
    "for id_ in maskwarp_unique_ids:\n",
    "    #print (id_)\n",
    "    if id_ in dorsalMaps_unique:\n",
    "        pass\n",
    "    else:\n",
    "        idx = np.where(maskwarp==id_)\n",
    "        \n",
    "        for k in range(len(idx[0])):\n",
    "            maskwarp[idx[0][k],idx[1][k]] = np.nan\n",
    "\n",
    "        for k in range(len(idx[0])):\n",
    "            temp = maskwarp[max(0,idx[0][k]-1): idx[0][k]+2, max(0,idx[1][k]-1): idx[1][k]+2].ravel()\n",
    "            uniques = np.unique(temp, return_counts=True)\n",
    "            idx4 = np.argmax(uniques[1])\n",
    "            val = uniques[0][idx4]  # Replace values with most frequent nearby vals;\n",
    "            maskwarp[idx[0][k],idx[1][k]] = val\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "plt.imshow(maskwarp)\n",
    "print (np.unique(maskwarp))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/cat/dorsalMaps_id.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8d536e90c3ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load names/labels for data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/cat/dorsalMaps_id.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"All Allen IDS: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/cat/dorsalMaps_id.npy'"
     ]
    }
   ],
   "source": [
    "\n",
    "# load names/labels for data\n",
    "ids = np.load('/home/cat/dorsalMaps_id.npy').squeeze()\n",
    "print (\"All Allen IDS: \", ids.shape[0])\n",
    "import csv\n",
    "names = []\n",
    "with open('/home/cat/dorsalMaps_name.txt', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        names.append(row)\n",
    "        \n",
    "print (\"All Allen area names: \", len(names[1:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered area ids:  (39,)\n",
      "registered ids:  [ 0  8 15 21 29 36 43 50 57 64]\n",
      "searching for  0\n",
      "searching for  8\n",
      "searching for  15\n",
      "searching for  21\n",
      "searching for  29\n",
      "searching for  36\n",
      "searching for  43\n",
      "searching for  50\n",
      "searching for  57\n",
      "searching for  64\n",
      "searching for  71\n",
      "searching for  78\n",
      "searching for  92\n",
      "searching for  100\n",
      "searching for  107\n",
      "searching for  114\n",
      "searching for  121\n",
      "searching for  129\n",
      "searching for  136\n",
      "searching for  143\n",
      "searching for  150\n",
      "searching for  157\n",
      "searching for  164\n",
      "searching for  171\n",
      "searching for  178\n",
      "searching for  186\n",
      "searching for  198\n",
      "searching for  217\n",
      "searching for  249\n",
      "searching for  255\n",
      "searching for  261\n",
      "searching for  268\n",
      "searching for  275\n",
      "searching for  282\n",
      "searching for  295\n",
      "searching for  300\n",
      "searching for  301\n",
      "searching for  651\n",
      "searching for  653\n"
     ]
    }
   ],
   "source": [
    "# visualize annotated areas:\n",
    "registered_ids = np.int32(np.unique(maskwarp))\n",
    "print (\"Registered area ids: \", registered_ids.shape)\n",
    "print (\"registered ids: \", registered_ids[:10])\n",
    "\n",
    "ctr=1\n",
    "areas = [15, 21, 43, 57,150, 249, 255,261, 268, 275]\n",
    "area_masks = []\n",
    "names_array = []\n",
    "fig=plt.figure()\n",
    "#for k in range(25,registered_ids.shape[0],25):\n",
    "for k in range(0,registered_ids.shape[0],1):\n",
    "    ax=plt.subplot(4,10,ctr)\n",
    "    temp = maskwarp.copy()\n",
    "\n",
    "    print (\"searching for \", registered_ids[k])\n",
    "    idx = np.where(temp!=registered_ids[k])\n",
    "    temp*=1\n",
    "    temp[idx]=0\n",
    "    temp*=mask\n",
    "    if registered_ids[k] in areas:\n",
    "        area_masks.append(idx)\n",
    "        names_array.append(names[registered_ids[k]+1])\n",
    "    plt.imshow(temp)\n",
    "    \n",
    "    ctr+=1\n",
    "    # label stuff\n",
    "    \n",
    "    titles = ''\n",
    "    for p in range(len(names[registered_ids[k]+1])):\n",
    "        titles = titles + names[registered_ids[k]+1][p]+'\\n'\n",
    "    \n",
    "    plt.ylabel(str(k)+\" \"+str(registered_ids[k]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(titles,fontsize=8,y=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[['\"Primary', 'motor', 'area,', 'Layer', '2/3\"'], ['\"Secondary', 'motor', 'area,', 'layer', '2/3\"'], ['\"Primary', 'somatosensory', 'area,', 'lower', 'limb,', 'layer', '2/3\"'], ['\"Primary', 'somatosensory', 'area,', 'upper', 'limb,', 'layer', '2/3\"'], ['\"Primary', 'visual', 'area,', 'layer', '2/3\"'], ['\"Retrosplenial', 'area,', 'lateral', 'agranular', 'part,', 'layer', '2/3\"'], ['\"Retrosplenial', 'area,', 'dorsal', 'part,', 'layer', '2/3\"'], ['\"Retrosplenial', 'area,', 'ventral', 'part,', 'layer', '2/3\"'], ['\"Anterior', 'area,', 'layer', '2/3\"'], ['\"Rostrolateral', 'area,', 'layer', '2/3\"']]\n"
     ]
    }
   ],
   "source": [
    "print (len(names_array))\n",
    "print (names_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cat/.local/lib/python3.8/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "# grab masks for RS dorsal and RS ventral \n",
    "img = np.zeros((128,128),'float32') #*np.nan\n",
    "print (len(area_masks))\n",
    "masks_2D = []\n",
    "for k in range(len(area_masks)):\n",
    "    print (k)\n",
    "    ax=plt.subplot(4,3,k+1)\n",
    "    temp =img.copy()\n",
    "    temp[area_masks[k]]=np.nan\n",
    "    masks_2D.append(temp)\n",
    "    plt.imshow(temp)\n",
    "    plt.title(names_array[k])\n",
    "plt.show()\n",
    "\n",
    "np.save('/home/cat/areas.npy', masks_2D)\n",
    "np.save('/home/cat/area_names.npy',names_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cat/.local/lib/python3.8/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 390, 128, 128)\n",
      "searching for area  8\n",
      "(390,)\n",
      "searching for area  15\n",
      "(390,)\n",
      "searching for area  21\n",
      "(390,)\n",
      "searching for area  29\n",
      "(390,)\n",
      "searching for area  36\n",
      "(390,)\n",
      "searching for area  43\n",
      "(390,)\n",
      "searching for area  50\n",
      "(390,)\n",
      "searching for area  57\n",
      "(390,)\n",
      "searching for area  64\n",
      "(390,)\n",
      "searching for area  71\n",
      "(390,)\n",
      "searching for area  78\n",
      "(390,)\n",
      "searching for area  92\n",
      "(390,)\n",
      "searching for area  100\n",
      "(390,)\n",
      "searching for area  107\n",
      "(390,)\n",
      "searching for area  114\n",
      "(390,)\n",
      "searching for area  121\n",
      "(390,)\n",
      "searching for area  129\n",
      "(390,)\n",
      "searching for area  136\n",
      "(390,)\n",
      "searching for area  143\n",
      "(390,)\n",
      "searching for area  150\n",
      "(390,)\n",
      "searching for area  157\n",
      "(390,)\n",
      "searching for area  164\n",
      "(390,)\n",
      "searching for area  171\n",
      "(390,)\n",
      "searching for area  178\n",
      "(390,)\n",
      "searching for area  186\n",
      "(390,)\n",
      "searching for area  198\n",
      "(390,)\n",
      "searching for area  217\n",
      "(390,)\n",
      "searching for area  249\n",
      "(390,)\n",
      "searching for area  255\n",
      "(390,)\n",
      "searching for area  261\n",
      "(390,)\n",
      "searching for area  268\n",
      "(390,)\n",
      "searching for area  275\n",
      "(390,)\n",
      "searching for area  282\n",
      "(390,)\n",
      "searching for area  295\n",
      "(390,)\n",
      "searching for area  300\n",
      "(390,)\n",
      "searching for area  301\n",
      "(390,)\n",
      "searching for area  651\n",
      "(390,)\n",
      "searching for area  653\n",
      "(390,)\n",
      "(390, 128, 128)\n",
      "final:  (128, 50048)\n"
     ]
    }
   ],
   "source": [
    "# USE stacks_ave_arrays ([# features, # times steps, width, height]) to visualize specific areas:\n",
    "# to plot ROI vs time\n",
    "\n",
    "temp_temp = np.array(stacks_ave_arrays)\n",
    "print (temp_temp.shape)\n",
    "\n",
    "selected_feature = 0\n",
    "t=np.arange(temp_temp.shape[1])/30. - 3\n",
    "ax = plt.subplot(2,1,1)\n",
    "for k in range(1,registered_ids.shape[0],1):\n",
    "\n",
    "    temp = maskwarp.copy()\n",
    "    \n",
    "    print (\"searching for area \", registered_ids[k])\n",
    "    # find indexes for particaulr ROI\n",
    "    idx = np.where(temp==registered_ids[k])\n",
    "    #time_series = temp_temp[selected_feature,:, :].mean(1).mean(1)#, idx[0], idx[1]].mean(0)\n",
    "    time_series = temp_temp[selected_feature, :, idx[0], idx[1]].mean(0)\n",
    "    print (time_series.shape)\n",
    "    \n",
    "    #for p in range(len(names[registered_ids[k]])):\n",
    "    #    if \"Retrosplenial\" in names[registered_ids[k]][p]:\n",
    "    plt.plot(t, time_series, label=names[registered_ids[k]])\n",
    "\n",
    "#plt.legend(fontsize=10)\n",
    "ax = plt.subplot(2,1,2)\n",
    "time_series = temp_temp[selected_feature]\n",
    "print (time_series.shape)\n",
    "temp_show = np.zeros((128,128),'float32')\n",
    "for k in range(time_series.shape[0]):\n",
    "    #print (time_series[k].shape, mask.shape, (time_series[k]*mask).shape)\n",
    "    temp_show= np.hstack((temp_show, time_series[k]*mask))\n",
    "\n",
    "#temp_show = #np.hstack(temp_show)\n",
    "print (\"final: \", temp_show.shape)\n",
    "plt.imshow(temp_show,aspect='auto')\n",
    "    \n",
    "    \n",
    "plt.suptitle(\"Trigger: \"+str(labels[selected_feature]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   8  15  16  20  21  29  36  43  50  57  64  71  78 100 114 129 136\n",
      " 143 150 157 164 171 178 186 198 249 255 261 268 275 282 300 301 653]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840\n",
      "0 ['name']\n",
      "8 ['Frontal pole, layer 1']\n",
      "15 ['Primary motor area, Layer 1']\n",
      "16 ['Primary motor area, Layer 2/3']\n",
      "20 ['Secondary motor area']\n",
      "21 ['Secondary motor area, layer 1']\n",
      "29 ['Primary somatosensory area, nose, layer 1']\n",
      "36 ['Primary somatosensory area, barrel field, layer 1']\n",
      "43 ['Primary somatosensory area, lower limb, layer 1']\n",
      "50 ['Primary somatosensory area, mouth, layer 1']\n",
      "57 ['Primary somatosensory area, upper limb, layer 1']\n",
      "64 ['Primary somatosensory area, trunk, layer 1']\n",
      "71 ['Primary somatosensory area, unassigned, layer 1']\n",
      "78 ['Supplemental somatosensory area, layer 1']\n",
      "100 ['Dorsal auditory area, layer 1']\n",
      "114 ['Posterior auditory area, layer 1']\n",
      "129 ['Anterolateral visual area, layer 1']\n",
      "136 ['Anteromedial visual area, layer 1']\n",
      "143 ['Lateral visual area, layer 1']\n",
      "150 ['Primary visual area, layer 1']\n",
      "157 ['Posterolateral visual area, layer 1']\n",
      "164 ['posteromedial visual area, layer 1']\n",
      "171 ['Laterointermediate area, layer 1']\n",
      "178 ['Postrhinal area, layer 1']\n",
      "186 ['Anterior cingulate area, dorsal part, layer 1']\n",
      "198 ['Prelimbic area, layer 1']\n",
      "249 ['Retrosplenial area, lateral agranular part, layer 1']\n",
      "255 ['Retrosplenial area, dorsal part, layer 1']\n",
      "261 ['Retrosplenial area, ventral part, layer 1']\n",
      "268 ['Anterior area, layer 1']\n",
      "275 ['Rostrolateral area, layer 1']\n",
      "282 ['Temporal association areas, layer 1']\n",
      "300 ['Olfactory areas']\n",
      "301 ['Main olfactory bulb']\n",
      "653 ['Paratrigeminal nucleus']\n"
     ]
    }
   ],
   "source": [
    "# load the ROI areas used for SVM analysis\n",
    "ROI_area_ids = np.load('/media/cat/4TBSSD/yuki/IA1/tif_files/IA1am_Mar4_30Hz/IA1am_Mar4_30Hz_code_04_lockout_10sec_trial_ROItimeCourses_10sec_area_ids.npy')\n",
    "print (ROI_area_ids)\n",
    "\n",
    "# load the names of all 840 allen ROIs\n",
    "fname = '/media/cat/4TBSSD/yuki/meta_allenmap/dorsalMaps_name.txt'\n",
    "\n",
    "# get the names of the ROI_area_ids\n",
    "import csv\n",
    "names = []\n",
    "with open(fname) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        names.append(row)\n",
    "\n",
    "# print the matched ids\n",
    "names=np.array(names)\n",
    "for id_ in ROI_area_ids:\n",
    "    print (id_, names[id_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make visualizatoin of all the ROI areas\n",
    "\n",
    "# set backend to not plot and save directly; allows for higher res figs\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# load the maskwarp we used\n",
    "mask = np.load('/media/cat/4TBSSD/yuki/meta_allenmap/maskwarp.npy')\n",
    "\n",
    "# plot data\n",
    "ctr=1\n",
    "fig=plt.figure(figsize=(30,30))\n",
    "for k in ROI_area_ids:\n",
    "    ax=plt.subplot(5,7,ctr)\n",
    "    idx = np.where(mask==k)\n",
    "    mask1 = mask.copy()\n",
    "    mask1[idx]=np.nan\n",
    "    plt.imshow(mask1)\n",
    "    ctr+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(str(k)+\" \"+str(names[k]), fontsize=9)\n",
    "plt.savefig('/home/cat/maps.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
