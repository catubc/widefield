{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import numpy as np\n",
    "\n",
    "# FUNCTION TO COMPUTE DFF\n",
    "import os\n",
    "from utility_classification import sum_pixels_in_registered_mask, fix_trials\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "from scipy.signal import butter, filtfilt, cheby1\n",
    "\n",
    "#select code 04/02/07 triggers;\n",
    "def get_04_triggers_with_lockout(root_dir, recording, lockout_window):\n",
    "    \n",
    "    # make sure locs\n",
    "    try:\n",
    "        locs_44threshold = np.load(root_dir + '/tif_files/' + recording + '/' + recording + '_locs44threshold.npy')\n",
    "    except:\n",
    "        print (\"locs 44 thrshold missing\", recording)\n",
    "        return np.zeros((0),'float32'), np.zeros((0),'float32')\n",
    "        \n",
    "    codes = np.load(root_dir + '/tif_files/' + recording + '/'+recording + '_code44threshold.npy')\n",
    "    code = b'04'\n",
    "    idx = np.where(codes==code)[0]\n",
    "    locs_selected = locs_44threshold[idx]\n",
    "\n",
    "    if locs_selected.shape[0]==0:\n",
    "        return np.zeros((0),'float32'), np.zeros((0),'float32')\n",
    "\n",
    "    diffs = locs_selected[1:]-locs_selected[:-1]\n",
    "    idx = np.where(diffs>lockout_window)[0]\n",
    "    \n",
    "    locs_selected_with_lockout = locs_selected[idx+1] \n",
    "    if locs_selected_with_lockout.shape[0]==0:\n",
    "        return np.zeros((0),'float32'), np.zeros((0),'float32')\n",
    "\n",
    "    # ADD FIRST VAL\n",
    "    if locs_selected[0]>lockout_window:\n",
    "        locs_selected_with_lockout = np.concatenate(([locs_selected[0]], locs_selected_with_lockout), axis=0)\n",
    "\n",
    "    # save data\n",
    "    np.savetxt(root_dir + '/tif_files/' + recording + '/'+recording+ \"_all_locs_selected.txt\" , \n",
    "               locs_selected)\n",
    "    np.savetxt(root_dir + '/tif_files/' + recording + '/'+recording+ \"_lockout_\"+str(lockout_window)+\n",
    "               \"sec_locs_selected.txt\" , locs_selected_with_lockout)\n",
    "    \n",
    "    return locs_selected, locs_selected_with_lockout\n",
    "    \n",
    "\n",
    "def find_nearest(array, value):\n",
    "    return (np.abs(array-value)).argmin()\n",
    "\n",
    "def load_reclength(filename):\n",
    "    \"\"\" Load realtime length of a single session. Probably should be in session, but was quicker to dump here\"\"\"\n",
    "\n",
    "    #print (\"FILENAME: \", filename)\n",
    "    text_file = open(filename, \"r\")\n",
    "    lines = text_file.read().splitlines()\n",
    "    event_text = []\n",
    "    for line in lines:\n",
    "        event_text.append(re.split(r'\\t+',line))\n",
    "\n",
    "    #Delete false starts from event file\n",
    "    for k in range(len(event_text)-1,-1,-1):        #Search backwards for the 1st occurence of \"date\" indicating last imaging start\n",
    "                                                    #NB: There can be multiple false starts WHICH DON\"T LINE UP - NEED TO IGNORE SUCH SESSIONS\n",
    "        if event_text[k][0]=='date': \n",
    "            event_text = event_text[k+2:]         #Remove first 2 lines\n",
    "            break\n",
    "\n",
    "    if len(event_text)==0:\n",
    "        reclength = 0\n",
    "    else:\n",
    "        if event_text[-1][2] != \"None\": \n",
    "            reclength = 0\n",
    "        else: \n",
    "            reclength = float(event_text[-1][3])\n",
    "\n",
    "    return reclength\n",
    "    \n",
    "\n",
    "# FUNCTION TO COMPUTE DFF\n",
    "def compute_DFF_function(\n",
    "                        root_dir,\n",
    "                        dff_method, # 'globalAverage' or 'slidingWindow'\n",
    "                        recording,\n",
    "                        locs_selected,\n",
    "                        n_sec_window\n",
    "                        ):\n",
    "\n",
    "    \n",
    "    # ###################################################\n",
    "    # ###################################################\n",
    "    # ###################################################\n",
    "    # SET DEFAULT PARAMETERS\n",
    "    #n_sec_window = 10\n",
    "    low_cut = 0.1\n",
    "    high_cut = 6.0\n",
    "    img_rate = np.loadtxt(root_dir+'/img_rate.txt')\n",
    "    selected_dff_filter = 'butterworth'\n",
    "\n",
    "    # MAKE FILENAMES\n",
    "    tif_files = root_dir+'/tif_files.npy'\n",
    "    event_files = root_dir + '/event_files.npy'\n",
    "    aligned_fname = root_dir + '/tif_files/'+recording + '/'+recording + \"_aligned.npy\"\n",
    "    #print (\"aligned fname: \", aligned_fname)\n",
    "\n",
    "    rec_filename = root_dir + '/tif_files/'+recording + '/'+recording +'.tif'\n",
    "    #print (\"rec_fileame;\", rec_filename)\n",
    "    n_sec = float(n_sec_window)\n",
    "        \n",
    "    # Load aligned/filtered data and find ON/OFF light;\n",
    "    #images_file = self.parent.animal.home_dir+self.parent.animal.name+'/tif_files/'+self.rec_filename+'/'+self.rec_filename+'_aligned.npy'\n",
    "    images_file = aligned_fname\n",
    "    try:\n",
    "        aligned_images = np.load(images_file)\n",
    "    except:\n",
    "        print (\"missing aligned images - skipping session\", recording)\n",
    "        return np.zeros((0),'float32')\n",
    "\n",
    "    \n",
    "    # Find blue light on/off \n",
    "    blue_light_threshold = 400  #Intensity threshold; when this value is reached - imaging light was turned on\n",
    "    start_blue = 0; end_blue = len(aligned_images)\n",
    "    \n",
    "    if np.average(aligned_images[0])> blue_light_threshold:    #Case #1: imaging starts with light on; need to remove end chunk; though likely bad recording\n",
    "        for k in range(len(aligned_images)):\n",
    "            if np.average(aligned_images[k])< blue_light_threshold:\n",
    "                #self.aligned_images = self.aligned_images[k:]\n",
    "                end_blue = k\n",
    "                break\n",
    "    else:                                                           #Case #2: start with light off; remove starting and end chunks;\n",
    "        #Find first light on\n",
    "        for k in range(len(aligned_images)):\n",
    "            if np.average(aligned_images[k])> blue_light_threshold:\n",
    "                start_blue = k\n",
    "                break\n",
    "\n",
    "        #Find light off - count backwards from end of imaging data\n",
    "        for k in range(len(aligned_images)-1,0,-1):\n",
    "            if np.average(aligned_images[k])> blue_light_threshold:\n",
    "                end_blue= k\n",
    "                break\n",
    "                \n",
    "                \n",
    "    #self.lowcut = float(self.parent.filter_low.text())\n",
    "    #self.highcut = float(self.parent.filter_high.text())\n",
    "        \n",
    "    #if self.selected_dff_filter == 'nofilter':\n",
    "    #    pass; #already loaded nonfiltered self.aligned_images above\n",
    "    #else:\n",
    "    filtered_filename = images_file[:-4]+'_'+selected_dff_filter+'_'+str(low_cut)+'hz_'+str(high_cut)+'hz.npy'\n",
    "    if os.path.exists(filtered_filename):\n",
    "        try:\n",
    "            aligned_images = np.load(filtered_filename, allow_pickle=True)\n",
    "        except:\n",
    "            print (\"aligned filtered images corrupt... recomputing: \", filtered_filename)\n",
    "            filter_data(root_dir, recording)\n",
    "            aligned_images = np.load(filtered_filename)\n",
    "    else:\n",
    "        print (\"aligned filtered images missing... recomputing: \", filtered_filename)\n",
    "        filter_data(root_dir, recording)\n",
    "        aligned_images = np.load(filtered_filename)\n",
    "        \n",
    "    aligned_images = aligned_images[start_blue:end_blue]\n",
    "    \n",
    "    # compute # of images in stack\n",
    "    n_images=len(aligned_images)\n",
    "\n",
    "\n",
    "    # Determine if imaging rate correct \n",
    "    temp_tif_files = np.load(tif_files)\n",
    "    temp_event_files = np.load(event_files)\n",
    "    if len(temp_event_files)==1:\n",
    "        temp_event_files = temp_event_files[0]\n",
    "    #print (\"temp_tif files;l\", temp_tif_files)\n",
    "    #print (\"rec_filename: \", rec_filename)\n",
    "\n",
    "    index = None\n",
    "    for k in range(len(temp_tif_files)):\n",
    "        try:\n",
    "            temp_temp = temp_tif_files[k].decode(\"utf-8\").replace('12TB/in_vivo/tim','4TBSSD').replace(\n",
    "                                                    '10TB/in_vivo/tim','4TBSSD')#.replace(\"b'/\", \"'/\")\n",
    "        except:\n",
    "            temp_temp = temp_tif_files[k].replace('12TB/in_vivo/tim','4TBSSD').replace(\n",
    "                                                    '10TB/in_vivo/tim','4TBSSD')#.replace(\"b'/\", \"'/\")\n",
    "        if rec_filename in temp_temp:\n",
    "            index = k \n",
    "            break\n",
    "    \n",
    "    if index == None:\n",
    "        \n",
    "        print (\"DID NOT FIND MATCH between imaging and lever ---- RETURNING \")\n",
    "        #print (temp_tif_files)\n",
    "        return np.zeros((0),'float32')\n",
    "    \n",
    "    #print (\"INDEX: \", index)\n",
    "    #print (\"temp event files indexed: \", len(temp_event_files[index]))\n",
    "    # load the reclength based \n",
    "    try:\n",
    "        reclength = load_reclength(temp_event_files[index].decode(\"utf-8\").replace(\n",
    "                                                    '10TB/in_vivo/tim','4TBSSD'))\n",
    "    except:\n",
    "        reclength = load_reclength(temp_event_files[index].replace(\n",
    "                                                    '10TB/in_vivo/tim','4TBSSD'))\n",
    "\n",
    "    if reclength ==0:\n",
    "        print (\"zero length recording exiting (excitation light failure)\", recording)\n",
    "        return np.zeros((0),'float32')\n",
    "    \n",
    "    # compute imaging rate; \n",
    "    session_img_rate = n_images/reclength\n",
    "\n",
    "    if abs(session_img_rate-float(img_rate))<0.01:         #Compare computed session img_rate w. experimentally set img_rate\n",
    "        np.save(images_file.replace('_aligned.npy','')+'_img_rate', session_img_rate)\n",
    "    else:\n",
    "        np.save(images_file.replace('_aligned.npy','')+'_img_rate', session_img_rate)\n",
    "        print (\"Imaging rates between aligned and session are incorrect, exiting: \", session_img_rate)\n",
    "        return np.zeros((0),'float32')\n",
    "\n",
    "\n",
    "    # Find times of triggers from lever pull threshold times\n",
    "    trigger_times = locs_selected\n",
    "    frame_times = np.linspace(0, reclength, n_images)             #Divide up reclength in number of images\n",
    "    img_frame_triggers = []\n",
    "    for i in range(len(trigger_times)):\n",
    "        #img_frame_triggers.append(self.find_previous(frame_times, trigger_times[i])) \n",
    "        img_frame_triggers.append(find_nearest(frame_times, trigger_times[i]))     #Two different functions possible here; \n",
    "    \n",
    "    #BASELINE FOR GLOBAL BASELINE REMOVAL\n",
    "    mean_file = root_dir + '/tif_files/'+recording + '/'+recording + '_aligned_mean.npy'\n",
    "    if os.path.exists(mean_file)==False:\n",
    "        aligned_fname = root_dir + '/tif_files/'+recording + '/'+recording + \"_aligned.npy\"\n",
    "        images_file = aligned_fname\n",
    "        images_aligned = np.load(images_file)\n",
    "        images_aligned_mean = np.mean(images_aligned, axis=0)\n",
    "        np.save(images_file[:-4]+'_mean', images_aligned_mean)\n",
    "\n",
    "    global_mean = np.load(mean_file)\n",
    "\n",
    "    abstimes = np.load(root_dir + '/tif_files/'+recording + '/'+recording + '_abstimes.npy')\n",
    "    abspositions = np.load(root_dir + '/tif_files/'+recording + '/'+recording + '_abspositions.npy')\n",
    "\n",
    "    data_stm = []; traces = []; locs = []; codes = []\n",
    "    counter=-1\n",
    "    window = n_sec * session_img_rate      #THIS MAY NOT BE GOOD ENOUGH; SHOULD ALWAYS GO BACK AT LEAST X SECONDS EVEN IF WINDOW IS ONLY 1SEC or 0.5sec...\n",
    "                                                            #Alternatively: always compute using at least 3sec window, and then just zoom in\n",
    "    for trigger in img_frame_triggers:\n",
    "        counter+=1\n",
    "        #NB: Ensure enough space for the sliding window; usually 2 x #frames in window\n",
    "        if trigger < (2*window) or trigger>(n_images-window): \n",
    "            continue  #Skip if too close to start/end\n",
    "\n",
    "        #add locs and codes\n",
    "        #locs.append(locs_44threshold_selected[counter])\n",
    "        #codes.append(code_44threshold_selected[counter])\n",
    "\n",
    "        # load data chunk working with\n",
    "        data_chunk = aligned_images[int(trigger-window):int(trigger+window)]\n",
    "\n",
    "        if dff_method == 'globalAverage':\n",
    "            data_stm.append((data_chunk-global_mean)/global_mean)    #Only need to divide by global mean as original data_chunk did not have mean img added in\n",
    "            \n",
    "        elif dff_method == 'slidingWindow':            #Use baseline -2*window .. -window\n",
    "            baseline = np.average(aligned_images[int(trigger-2*window):int(trigger-window)], axis=0)\n",
    "            data_stm.append((data_chunk-baseline)/baseline)\n",
    "        \n",
    "        #***PROCESS TRACES - WORKING IN DIFFERENT TIME SCALE\n",
    "        lever_window = int(120*n_sec)    #NB: Lever window is computing in real time steps @ ~120Hz; and discontinuous;\n",
    "        t = np.linspace(-lever_window*0.0082,\n",
    "                        lever_window*0.0082, \n",
    "                        lever_window*2)\n",
    "        #lever_position_index = find_nearest(np.array(self.abstimes), self.locs_44threshold[counter])\n",
    "        lever_position_index = find_nearest(np.array(abstimes), locs_selected[counter])\n",
    "        \n",
    "        lever_trace = abspositions[int(lever_position_index-lever_window):int(lever_position_index+lever_window)]\n",
    "\n",
    "        if len(lever_trace)!=len(t):    #Extraplote missing data\n",
    "            lever_trace = np.zeros(lever_window*2,dtype=np.float32)\n",
    "            for k in range(-lever_window,lever_window,1):\n",
    "                lever_trace[k+lever_window] = self.abspositions[k+lever_window]     #Double check this...\n",
    "\n",
    "        traces.append(lever_trace)\n",
    "\n",
    "    data_stm = np.array(data_stm)\n",
    "    \n",
    "    return data_stm\n",
    "\n",
    "\n",
    "def filter_data(root_dir,\n",
    "                recording,\n",
    "                ):\n",
    "\n",
    "    \n",
    "    # ###################################################\n",
    "    # ###################################################\n",
    "    # ###################################################\n",
    "    # SET DEFAULT PARAMETERS\n",
    "    #n_sec_window = 10\n",
    "    low_cut = 0.1\n",
    "    high_cut = 6.0\n",
    "    img_rate = 30.0\n",
    "    selected_dff_filter = 'butterworth'\n",
    "\n",
    "    # MAKE FILENAMES\n",
    "    generic_mask_fname = root_dir + '/genericmask.txt'\n",
    "    tif_files = root_dir+'tif_files.npy'\n",
    "    event_files = root_dir + 'event_files.npy'\n",
    "    aligned_fname = root_dir + '/tif_files/'+recording + '/'+recording + \"_aligned.npy\"\n",
    "    \n",
    "    #print (\"FILTERING DATA: \", aligned_fname)\n",
    "    \n",
    "    # FILTERING STEP\n",
    "    images_file = aligned_fname\n",
    "    \n",
    "    filter_type = selected_dff_filter\n",
    "    lowcut = low_cut\n",
    "    highcut = high_cut\n",
    "    fs = img_rate\n",
    "\n",
    "    #Check to see if data requested exists- THIS CHECK WAS ALREADY DONE PRIOR TO ENTERING FUNCTION\n",
    "    if False:\n",
    "        if os.path.exists(images_file[:-4]+'_'+filter_type+'_'+str(lowcut)+'hz_'+str(highcut)+'hz.npy'):\n",
    "            #print (\"filtered data already exists...\")\n",
    "            return\n",
    "\n",
    "    #Load aligned images\n",
    "    if os.path.exists(images_file):\n",
    "        images_aligned = np.load(images_file)\n",
    "    else:\n",
    "        print (\" ...missing aligned images... NEED TO RUN ALIGN ALGORITHMS\", images_file)\n",
    "        return None\n",
    "        \n",
    "        # TODO IMPLMENET ALIGNMENT TOOL\n",
    "        #images_aligned = align_images2(self)\n",
    "        \n",
    "    #Save mean of images_aligned if not already done\n",
    "    if os.path.exists(images_file[:-4]+'_mean.npy')==False: \n",
    "        images_aligned_mean = np.mean(images_aligned, axis=0)\n",
    "        np.save(images_file[:-4]+'_mean', images_aligned_mean)\n",
    "    else:\n",
    "        images_aligned_mean = np.load(images_file[:-4]+'_mean.npy')\n",
    "            \n",
    "    #Load mask - filter only datapoints inside mask\n",
    "    n_pixels = len(images_aligned[0])\n",
    "    generic_coords = np.loadtxt(generic_mask_fname)\n",
    "    generic_mask_indexes=np.zeros((n_pixels,n_pixels))\n",
    "    for i in range(len(generic_coords)): generic_mask_indexes[int(generic_coords[i][0])][int(generic_coords[i][1])] = True\n",
    "\n",
    "    #Filter selection and parameters\n",
    "    if filter_type == 'butterworth':\n",
    "        nyq = 0.5 * fs\n",
    "        low = lowcut / nyq\n",
    "        high = highcut / nyq\n",
    "        order = 2\n",
    "        b, a = butter(order, [low, high], btype='band')\n",
    "    elif filter_type == 'chebyshev':\n",
    "        nyq = fs / 2.0\n",
    "        order = 4\n",
    "        rp = 0.1\n",
    "        Wn = [lowcut / nyq, highcut / nyq]\n",
    "        b, a = cheby1(order, rp, Wn, 'bandpass', analog=False)\n",
    "    \n",
    "    \n",
    "    #Load individual pixel time courses; SWITCH TO UNRAVEL HERE****\n",
    "    import time\n",
    "   \n",
    "    filtered_array = np.zeros(images_aligned.shape, dtype=np.float16)\n",
    "    now = time.time(); start_time = now\n",
    "    cutoff=n_pixels\n",
    "    #from tqdm import tqdm\n",
    "    #for p1 in tqdm(range(n_pixels)):\n",
    "    for p1 in range(n_pixels):\n",
    "        now=time.time(); n_pixels_in=0\n",
    "        for p2 in range(n_pixels):\n",
    "            if generic_mask_indexes[p1,p2]==False:\n",
    "                filtered_array[:,p1,p2] = np.float16(filtfilt(b, a, images_aligned[:,p1,p2])); n_pixels_in+=1   #filter pixel inside mask\n",
    "        \n",
    "    np.save(images_file[:-4]+'_'+filter_type+'_'+str(lowcut)+'hz_'+str(highcut)+'hz', \n",
    "            filtered_array+np.float16(images_aligned_mean))\n",
    "\n",
    "    return \n",
    "\n",
    "def compute_trial_courses_ROI(recording, root_dir):\n",
    "\n",
    "    #try: \n",
    "        fname_04 = root_dir + '/tif_files/' + recording + \"_data_04_code_trial_timeCourses.npy\"\n",
    "        fname_random = root_dir + '/tif_files/' + recording + \"_data_random_code_trial_timeCourses.npy\"\n",
    "\n",
    "        # SET PARAMETERS\n",
    "        n_sec_window = 10\n",
    "        dff_method = 'globalAverage'\n",
    "\n",
    "        if os.path.exists(fname_04)==False:\n",
    "            # select code 04/02/07 triggers;\n",
    "            try:\n",
    "                locs_44threshold = np.load(root_dir + '/tif_files/' + recording + '/' + recording + '_locs44threshold.npy')\n",
    "            except:\n",
    "                print (\"locs 44 thrshold missing\", recording)\n",
    "                return None\n",
    "            \n",
    "            codes = np.load(root_dir + '/tif_files/' + recording + '/'+recording + '_code44threshold.npy')\n",
    "            code = b'04'\n",
    "            idx = np.where(codes==code)[0]\n",
    "            locs_selected = locs_44threshold[idx]\n",
    "\n",
    "            # CALL FUNCTION;\n",
    "            data_stm = compute_DFF_function(\n",
    "                                    root_dir,\n",
    "                                    dff_method, # 'globalAverage' or 'slidingWindow'\n",
    "                                    recording,\n",
    "                                    locs_selected,\n",
    "                                    n_sec_window\n",
    "                                    )\n",
    "            if data_stm is None:\n",
    "                print (\"data_stm is None\", recording)\n",
    "                return\n",
    "\n",
    "\n",
    "            # CONVERT DATA FROM 128 x 128 to 35 ROIs\n",
    "\n",
    "            # load Allen Institute afine transformation to scale data\n",
    "            #maskwarp= np.load('/media/cat/4TBSSD/yuki/IA2/tif_files/IA2pm_Apr22_Week2_30Hz/IA2pm_Apr22_Week2_30Hz_aligned_maskwarp.npy')\n",
    "            maskwarp = np.load('/home/cat/maskwarp.npy')\n",
    "\n",
    "            # accumulate mean activity in each ROI\n",
    "            # input data shape: [# trials, # times, width, height]\n",
    "            area_ids, trial_courses = sum_pixels_in_registered_mask(data_stm, maskwarp)\n",
    "\n",
    "            # generate random time corses\n",
    "            locs_selected = np.float32(np.linspace(30, 1200, data_stm.shape[0]))\n",
    "            locs_selected = locs_selected + np.random.rand(locs_selected.shape[0])*10-5      \n",
    "\n",
    "            # CALL FUNCTION;\n",
    "            data_stm_random = compute_DFF_function(\n",
    "                                    root_dir,\n",
    "                                    dff_method, # 'globalAverage' or 'slidingWindow'\n",
    "                                    recording,\n",
    "                                    locs_selected,\n",
    "                                    n_sec_window\n",
    "                                    )\n",
    "            if data_stm_random is None:\n",
    "                return\n",
    "\n",
    "            # compute random trial time courses\n",
    "            _, trial_courses_random = sum_pixels_in_registered_mask(data_stm_random, maskwarp)\n",
    "\n",
    "            #####################################################################\n",
    "            ######## REMOVE INFINITIES, NANS ETC FROM DATA ######################\n",
    "            #####################################################################\n",
    "            if trial_courses.shape[0]==0:\n",
    "                return\n",
    "            \n",
    "            trial_courses_fixed, trial_courses_random_fixed = fix_trials(trial_courses, trial_courses_random)\n",
    "\n",
    "            np.save(fname_04+ \"_area_ids.npy\", area_ids)\n",
    "            np.save(fname_04, trial_courses_fixed)\n",
    "            #print (\"Saved DFF for triggered data: \", fname_04, trial_courses_fixed.shape)\n",
    "\n",
    "            #offset = DLC_offset\n",
    "            np.save(fname_random, trial_courses_random_fixed)\n",
    "            #print (\"Saved DFF for random data: \", trial_courses_random_fixed.shape)\n",
    "    #except:\n",
    "    #    print (\"skipped: \", recording)\n",
    "    #    pass\n",
    "   # print ('')\n",
    "\n",
    "\n",
    "# def compute_trial_courses_ROI_any_trigger(recording, \n",
    "#                                           root_dir,\n",
    "#                                           locs_selected,\n",
    "#                                           feature_name,\n",
    "#                                           n_sec_window=10):\n",
    "\n",
    "\n",
    "#         # GENERATE SAVE FILENAMES \n",
    "#         fname_04 = root_dir + '/tif_files/' + recording + '/' + recording + \"_\"+feature_name+\"_trial_timeCourses.npy\"\n",
    "#         fname_random = root_dir + '/tif_files/' + recording + '/' + recording + \"_\"+feature_name+\"_random_trial_timeCourses.npy\"\n",
    "\n",
    "#         # SET PARAMETERS\n",
    "#         #n_sec_window = 10\n",
    "#         dff_method = 'globalAverage'\n",
    "\n",
    "#         if os.path.exists(fname_04)==False:\n",
    "\n",
    "#             # CALL FUNCTION;\n",
    "#             data_stm = compute_DFF_function(\n",
    "#                                     root_dir,\n",
    "#                                     dff_method, # 'globalAverage' or 'slidingWindow'\n",
    "#                                     recording,\n",
    "#                                     locs_selected,\n",
    "#                                     n_sec_window\n",
    "#                                     )\n",
    "#             if data_stm is None:\n",
    "#                 print (\"data_stm is None\", recording)\n",
    "#                 return\n",
    "\n",
    "\n",
    "#             # CONVERT DATA FROM 128 x 128 to 35 ROIs\n",
    "\n",
    "#             # load Allen Institute afine transformation to scale data\n",
    "#             #maskwarp= np.load('/media/cat/4TBSSD/yuki/IA2/tif_files/IA2pm_Apr22_Week2_30Hz/IA2pm_Apr22_Week2_30Hz_aligned_maskwarp.npy')\n",
    "#             maskwarp = np.load('/home/cat/maskwarp.npy')\n",
    "\n",
    "#             # accumulate mean activity in each ROI\n",
    "#             # input data shape: [# trials, # times, width, height]\n",
    "#             area_ids, trial_courses = sum_pixels_in_registered_mask(data_stm, maskwarp)\n",
    "\n",
    "#             # generate random time corses\n",
    "#             locs_selected = np.float32(np.linspace(30, 1200, data_stm.shape[0])) \n",
    "#             locs_selected = locs_selected + np.random.rand(locs_selected.shape[0])*10-5      \n",
    "\n",
    "#             # CALL FUNCTION;\n",
    "#             data_stm_random = compute_DFF_function(\n",
    "#                                     root_dir,\n",
    "#                                     dff_method, # 'globalAverage' or 'slidingWindow'\n",
    "#                                     recording,\n",
    "#                                     locs_selected,\n",
    "#                                     n_sec_window\n",
    "#                                     )\n",
    "#             if data_stm_random is None:\n",
    "#                 return\n",
    "\n",
    "#             # compute random trial time courses\n",
    "#             _, trial_courses_random = sum_pixels_in_registered_mask(data_stm_random, maskwarp)\n",
    "\n",
    "#             #####################################################################\n",
    "#             ######## REMOVE INFINITIES, NANS ETC FROM DATA ######################\n",
    "#             #####################################################################\n",
    "#             if trial_courses.shape[0]==0:\n",
    "#                 return\n",
    "            \n",
    "#             trial_courses_fixed, trial_courses_random_fixed = fix_trials(trial_courses, trial_courses_random)\n",
    "\n",
    "#             np.save(fname_04+ \"_area_ids.npy\", area_ids)\n",
    "#             np.save(fname_04, trial_courses_fixed)\n",
    "#             #print (\"Saved DFF for triggered data: \", fname_04, trial_courses_fixed.shape)\n",
    "\n",
    "#             #offset = DLC_offset\n",
    "#             np.save(fname_random, trial_courses_random_fixed)\n",
    "#             #print (\"Saved DFF for random data: \", trial_courses_random_fixed.shape)\n",
    "\n",
    "            \n",
    "\n",
    "def compute_trial_courses_ROI_code04_trigger(recording, \n",
    "                                              root_dir,\n",
    "                                              feature_name,\n",
    "                                              lockout_window,   # THIS IS THE LOCKOUT WINDOW FOR NO OTHER PULLS\n",
    "                                              n_sec_window,\n",
    "                                              recompute,\n",
    "                                              midline_filter_flag,\n",
    "                                              save_stm_flag,\n",
    "                                              use_transformed_stm_flag,\n",
    "                                              use_fixed_filter_flag,\n",
    "                                              fname_filter\n",
    "                                            ):   # THIS IS THE DFF TIEM COURSE WINDOW; e.g. -10..+10sec\n",
    "\n",
    "\n",
    "    # SET PARAMETERS\n",
    "    #n_sec_window = 10\n",
    "    dff_method = 'globalAverage'\n",
    "\n",
    "    locs_selected, locs_selected_with_lockout = get_04_triggers_with_lockout(root_dir, \n",
    "                                                                            recording,\n",
    "                                                                            lockout_window)\n",
    "\n",
    "    # GENERATE SAVE FILENAMES FOR ALL CODE_04 DATA\n",
    "    fname_04 = (root_dir + '/tif_files/' + recording + '/' + recording + \"_\"+feature_name+\n",
    "                \"_trial_ROItimeCourses_\"+str(n_sec_window)+\"sec.npy\")\n",
    "\n",
    "    fname_random = (root_dir + '/tif_files/' + recording + '/' + recording + \"_\"+feature_name+\n",
    "                    \"_random_ROItimeCourses_\"+str(n_sec_window)+\"sec.npy\")\n",
    "\n",
    "    # good idea to save these as text to see them after:\n",
    "    np.savetxt(fname_04[:-4]+\"_locs_selected.txt\" , locs_selected)\n",
    "\n",
    "    \n",
    "    #if os.path.exists(fname_04)==False:\n",
    "    dff1, dff1_random = generate_arrays_ROI_triggered(root_dir,\n",
    "                                                         dff_method,\n",
    "                                                         recording,\n",
    "                                                         locs_selected,\n",
    "                                                         n_sec_window,\n",
    "                                                         fname_04,\n",
    "                                                         fname_random,\n",
    "                                                         recompute,\n",
    "                                                         midline_filter_flag,\n",
    "                                                         save_stm_flag,\n",
    "                                                         transform_data_flag,\n",
    "                                                         use_fixed_filter_flag,\n",
    "                                                         fname_filter\n",
    "                                                     )\n",
    "\n",
    "    # GENERATE SAVE FILENAMES FOR LOCKOUT DATA\n",
    "    fname_04 = (root_dir + '/tif_files/' + recording + '/' + recording + \"_\"+feature_name+\n",
    "                \"_lockout_\"+str(lockout_window)+\"sec_trial_ROItimeCourses_\"+str(n_sec_window)+\"sec.npy\")\n",
    "    fname_random = (root_dir + '/tif_files/' + recording + '/' + recording + \"_\"+feature_name+\n",
    "                    \"_lockout_\"+str(lockout_window)+\"sec_random_ROItimeCourses_\"+str(n_sec_window)+\"sec.npy\")\n",
    "\n",
    "    #if os.path.exists(fname_04)==False:\n",
    "    dff2, dff2_random = generate_arrays_ROI_triggered(root_dir,\n",
    "                                                         dff_method,\n",
    "                                                         recording,\n",
    "                                                         locs_selected_with_lockout,\n",
    "                                                         n_sec_window,\n",
    "                                                         fname_04,\n",
    "                                                         fname_random,\n",
    "                                                         recompute,\n",
    "                                                         midline_filter_flag,\n",
    "                                                         save_stm_flag,\n",
    "                                                         transform_data_flag,\n",
    "                                                         use_fixed_filter_flag,\n",
    "                                                         fname_filter)\n",
    "\n",
    "\n",
    "    return dff1, dff1_random, dff2, dff2_random\n",
    "\n",
    "\n",
    "\n",
    "def load_trial_courses_ROI_code04_trigger(recording, \n",
    "                                          root_dir,\n",
    "                                          feature_name,\n",
    "                                          lockout_window,   # THIS IS THE LOCKOUT WINDOW FOR NO OTHER PULLS\n",
    "                                          n_sec_window):   # THIS IS THE DFF TIEM COURSE WINDOW; e.g. -10..+10sec\n",
    "\n",
    "\n",
    "    # GENERATE SAVE FILENAMES FOR ALL CODE_04 DATA\n",
    "    fname_04 = (root_dir + '/tif_files/' + recording + '/' + recording + \"_\"+feature_name+\n",
    "                \"_trial_ROItimeCourses_\"+str(n_sec_window)+\"sec.npy\")\n",
    "\n",
    "    fname_random = (root_dir + '/tif_files/' + recording + '/' + recording + \"_\"+feature_name+\n",
    "                    \"_random_ROItimeCourses_\"+str(n_sec_window)+\"sec.npy\")\n",
    "\n",
    "    data_04 = np.load(fname_04)\n",
    "    data_04_random = np.load(fname_random)\n",
    "\n",
    "        \n",
    "    # GENERATE SAVE FILENAMES FOR LOCKOUT DATA\n",
    "    fname_04 = (root_dir + '/tif_files/' + recording + '/' + recording + \"_\"+feature_name+\n",
    "                \"_lockout_\"+str(lockout_window)+\"sec_trial_ROItimeCourses_\"+str(n_sec_window)+\"sec.npy\")\n",
    "    fname_random = (root_dir + '/tif_files/' + recording + '/' + recording + \"_\"+feature_name+\n",
    "                    \"_lockout_\"+str(lockout_window)+\"sec_random_ROItimeCourses_\"+str(n_sec_window)+\"sec.npy\")\n",
    "\n",
    "    \n",
    "    data_04_lockout = np.load(fname_04)\n",
    "    data_04_lockout_random = np.load(fname_random)\n",
    "    \n",
    "    \n",
    "    return data_04, data_04_random, data_04_lockout, data_04_lockout_random\n",
    "\n",
    "\n",
    "def compute_midline_filter(root_dir,\n",
    "                              data_stm):\n",
    "    import yaml\n",
    "\n",
    "    with open(os.path.join(root_dir,\"gcamp.txt\"), 'r') as f:\n",
    "        valuesYaml = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    val999 = valuesYaml['val999']\n",
    "    width = valuesYaml['width']\n",
    "    power = valuesYaml['power']\n",
    "    maxval = 10 # is not being used currently\n",
    "\n",
    "    # generate midline filter using t=0 to approximately t=0.5sec brain activity which most likely to generate midline blood pressure \n",
    "    # artifacts\n",
    "    #print (\"data stm: \", data_stm.shape)\n",
    "    midline_filter = motion_mask_parallel(data_stm.mean(0)[data_stm.shape[0]//2: \n",
    "                                                           data_stm.shape[0]//2+15].mean(0), \n",
    "                                                          maxval,  # this value is not used\n",
    "                                                          val999, \n",
    "                                                          width, \n",
    "                                                          power)\n",
    "\n",
    "    return midline_filter\n",
    "\n",
    "\n",
    "def correct_midline_artifact(data_stm,\n",
    "                            midline_filter,\n",
    "                            transform):\n",
    "    \n",
    "    # scale the midline filter by the saved value from the transfomr file\n",
    "    # loop over all frames in data stack and filter out midline activity\n",
    "    for k in range(data_stm.shape[0]):\n",
    "        for p in range(data_stm.shape[1]):\n",
    "            data_stm[k,p] *=midline_filter**transform[4]\n",
    "\n",
    "    return data_stm \n",
    "\n",
    "\n",
    "\n",
    "def rotate_translate_resize(transform, data):\n",
    "\n",
    "    from skimage.transform import resize, rotate\n",
    "\n",
    "    #data = np.float64(data)\n",
    "\n",
    "    # rotate\n",
    "    data = rotate(data, transform[2])\n",
    "\n",
    "    # roll\n",
    "    data = np.roll(data, int(transform[0]), axis=1)\n",
    "    data = np.roll(data, int(transform[1]), axis=0)\n",
    "\n",
    "    # resize\n",
    "    data = resize(data, (int(data.shape[0]*transform[3]),\n",
    "                         int(data.shape[1]*transform[3])))\n",
    "\n",
    "    # clip back down to 128 x 128\n",
    "    data = data[data.shape[0]//2-64:data.shape[0]//2+64,\n",
    "                data.shape[0]//2-64:data.shape[0]//2+64]\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def transform_data(transform, stm):\n",
    "    from tqdm import trange\n",
    "    for k in trange(stm.shape[0]):\n",
    "        for p in range(stm.shape[1]):\n",
    "            stm[k,p] = rotate_translate_resize(transform, stm[k,p])\n",
    "    \n",
    "    return stm\n",
    "                \n",
    "def generate_arrays_ROI_triggered(root_dir,\n",
    "                                 dff_method,\n",
    "                                 recording,\n",
    "                                 locs_selected,\n",
    "                                 n_sec_window,\n",
    "                                 fname_04,\n",
    "                                 fname_random, \n",
    "                                 recompute,\n",
    "                                 midline_filter_flag,\n",
    "                                 save_stm_flag,\n",
    "                                 transform_data_flag,\n",
    "                                 use_fixed_filter_flag,\n",
    "                                 fname_filter):\n",
    "     \n",
    "    from tqdm import trange\n",
    "    \n",
    "    # Compute DFF\n",
    "    data_stm = compute_DFF_function(\n",
    "                            root_dir,\n",
    "                            dff_method, # 'globalAverage' or 'slidingWindow'\n",
    "                            recording,\n",
    "                            locs_selected,\n",
    "                            n_sec_window\n",
    "                            )\n",
    "    \n",
    "    # return if DFF data is none\n",
    "    if data_stm.shape[0]==0:\n",
    "        print (\"data_stm is None\", recording)\n",
    "        return np.zeros((0), 'float32'), np.zeros((0), 'float32') \n",
    "\n",
    "    # try to load transform coordinates to assess filter power\n",
    "    try:\n",
    "        transform = np.load(os.path.split(fname_04)[0]+\"/transform.npy\")\n",
    "    except:\n",
    "        # set dummy values so the filter power can be set to 1 below\n",
    "        transform = [0,  # xtranslate\n",
    "                     0,  # ytranslate\n",
    "                     0,  # rotation\n",
    "                     1,  # resizing\n",
    "                     1]  # power of filter\n",
    "            \n",
    "    # Filter to remove midline artifacts etc.\n",
    "    if midline_filter_flag:\n",
    "        \n",
    "        if use_fixed_filter_flag:\n",
    "            midline_filter = np.load(fname_filter,allow_pickle='True')\n",
    "            print (\"using fixed filter\")\n",
    "            \n",
    "            # must also force the same filter on all the data;\n",
    "            transform = [0,  # xtranslate\n",
    "                         0,  # ytranslate\n",
    "                         0,  # rotation\n",
    "                         1,  # resizing\n",
    "                         1]  # power of filter\n",
    "\n",
    "        else:\n",
    "            midline_filter = compute_midline_filter(root_dir,\n",
    "                                          data_stm)\n",
    "            midline_filter.dump(fname_04[:-4]+ \"_midline_filter.npy\")\n",
    "\n",
    "        data_stm = correct_midline_artifact(data_stm, \n",
    "                                            midline_filter,\n",
    "                                            transform)  # pass in filter power value to decrease the amount of filtering if necessary\n",
    "        \n",
    "\n",
    "    # check to see if data requires TRANSOFMRATIONS: rotations, etc.\n",
    "    if transform_data_flag:\n",
    "        try:\n",
    "            transform = np.load(os.path.split(fname_04)[0]+\"/transform.npy\")\n",
    "            data_stm = transform_data(transform, data_stm)\n",
    "        except:\n",
    "            print (\"Transform coordinates missing ... skipping\")\n",
    "\n",
    "    # save data_stm stack\n",
    "    if save_stm_flag:\n",
    "        fname_04_data_stm = fname_04[:-4]+\"_all_brain.npy\"\n",
    "        np.save(fname_04_data_stm, data_stm)\n",
    "        \n",
    "\n",
    "    # CONVERT DATA FROM 128 x 128 to 35 ROIs\n",
    "\n",
    "    # load Allen Institute afine transformation to scale data\n",
    "    maskwarp = np.load('/media/cat/4TBSSD/yuki/maskwarp.npy')\n",
    "\n",
    "    # accumulate mean activity in each ROI\n",
    "    # input data shape: [# trials, # times, width, height]\n",
    "    area_ids, trial_courses = sum_pixels_in_registered_mask(data_stm, maskwarp)\n",
    "\n",
    "\n",
    "    ########################################################\n",
    "    ########## COMPUTE CONTROL/RANDOM DATA #################\n",
    "    ########################################################\n",
    "    # generate random time corses\n",
    "    locs_selected = np.float32(np.linspace(30, 1100, data_stm.shape[0])) \n",
    "    locs_selected = locs_selected + np.random.rand(locs_selected.shape[0])*10-5      \n",
    "    data_stm = None\n",
    "\n",
    "\n",
    "    # DFF for random data\n",
    "    data_stm_random = compute_DFF_function(\n",
    "                            root_dir,\n",
    "                            dff_method, # 'globalAverage' or 'slidingWindow'\n",
    "                            recording,\n",
    "                            locs_selected,\n",
    "                            n_sec_window\n",
    "                            )\n",
    "\n",
    "    if data_stm_random is None:\n",
    "        return np.zeros((0), 'float32'), np.zeros((0), 'float32') \n",
    "\n",
    "    # use the same filter as in the event triggered neural activity\n",
    "    if midline_filter_flag:\n",
    "\n",
    "        data_stm_random = correct_midline_artifact(data_stm_random,\n",
    "                                                   midline_filter,\n",
    "                                                   transform)\n",
    "\n",
    "    # transform random data: rotations, etc.\n",
    "    # check to see if data requires TRANSOFMRATIONS: rotations, etc.\n",
    "    if transform_data_flag:\n",
    "        try:\n",
    "            transform = np.load(os.path.split(fname_04)[0]+\"/transform.npy\")\n",
    "            data_stm_random = transform_data(transform, data_stm_random)\n",
    "        except:\n",
    "            print (\"Transform coordinates missing ... skipping\")\n",
    "       \n",
    "    # compute random trial time courses\n",
    "    _, trial_courses_random = sum_pixels_in_registered_mask(data_stm_random, maskwarp)\n",
    "    data_stm_random = None\n",
    "\n",
    "    #####################################################################\n",
    "    ######## REMOVE INFINITIES, NANS ETC FROM DATA ######################\n",
    "    #####################################################################\n",
    "    if trial_courses.shape[0]==0 or trial_courses_random.shape[0]==0:\n",
    "        return np.zeros((0), 'float32'), np.zeros((0), 'float32') \n",
    "\n",
    "    # remove infiities\n",
    "    trial_courses_fixed, trial_courses_random_fixed = fix_trials(trial_courses, trial_courses_random)\n",
    "\n",
    "    # save area ids, time courses for event triggered and random data\n",
    "    np.save(fname_04[:-4]+ \"_area_ids.npy\", area_ids)\n",
    "    np.save(fname_04, trial_courses_fixed)\n",
    "    np.save(fname_random, trial_courses_random_fixed)\n",
    "        \n",
    "    return trial_courses_fixed, trial_courses_random_fixed\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid_function(x, a, b):\n",
    "\n",
    "    return np.clip(a*(np.ma.log(x) - np.ma.log(1 - x))+b, 0, 1)      #Compute sigmoid and cut off values below 0 and above 1\n",
    "    \n",
    "    \n",
    "def mangle(width, x, img_temp, maxval, power, val999):\n",
    "    \n",
    "    mu = 0 #Select approximate midline as centre of gaussian\n",
    "    sig = width\n",
    "    \n",
    "    a = .005       #The steepness of the sigmoid function\n",
    "    b = val999        #% of maxval to cutoff\n",
    "\n",
    "\n",
    "    #Normalize img_temp for sigmoid to work properly\n",
    "    #img_temp_norm = (img_temp-np.min(img_temp))/(np.max(img_temp) - np.min(img_temp))\n",
    "    img_temp_norm = (img_temp-np.min(img_temp))/(np.max(img_temp) - np.min(img_temp))\n",
    "\n",
    "\n",
    "    #Original root function\n",
    "    #return -np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.))) * (abs(pix_val/maxval)**(1./power))\n",
    "    return -np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.))) * sigmoid_function(img_temp_norm, a, b)\n",
    "\n",
    "\n",
    "# FUNCTION TO MASK\n",
    "def motion_mask_parallel(img_temp, maxval, val999, width, power):\n",
    "    '''Parallel computation of mask\n",
    "    '''\n",
    "   \n",
    "    y_array = []\n",
    "    for x in range(len(img_temp)):\n",
    "        y_array.append(np.arange(0,len(img_temp), 1))\n",
    "        \n",
    "    y_array = np.vstack(y_array)\n",
    "    motion_mask = img_temp*mangle(width, np.abs(64-y_array), img_temp, maxval, power, val999)\n",
    "    \n",
    "                   \n",
    "    motion_mask = (motion_mask-np.min(motion_mask))/(np.max(motion_mask)-np.min(motion_mask))\n",
    "\n",
    "    idx = np.where(motion_mask==0)\n",
    "    motion_mask[idx]=1\n",
    "    \n",
    "    #motion_mask = motion_mask * img_temp\n",
    "        \n",
    "    return motion_mask\n",
    "\n",
    "def save_npz_data(save_dir,\n",
    "                  names, \n",
    "                  lockout_window,\n",
    "                  n_sec_window,\n",
    "                  feature_name,\n",
    "                  selected_sessions_animal, \n",
    "                  best_sessions_animal,\n",
    "                  save_best_flag):  #save only the best datasets\n",
    "\n",
    "    #feature_name = 'code_04'\n",
    "    #lockout_window = 10\n",
    "    #n_sec_window = 10\n",
    "\n",
    "    for name in names:\n",
    "\n",
    "        # \n",
    "        fname_out = save_dir+'/'+name+'.npz'\n",
    "\n",
    "        # \n",
    "        data_04_list = []\n",
    "        data_04_random_list = []\n",
    "        data_04_lockout_list = []\n",
    "        data_04_lockout_random_list = []\n",
    "\n",
    "        session_list = []\n",
    "        ctr_list = []\n",
    "\n",
    "\n",
    "        root_dir = '/media/cat/4TBSSD/yuki/'+name\n",
    "\n",
    "        temp_recs = np.load(root_dir+'/tif_files.npy')\n",
    "        recordings =[]\n",
    "        for k in range(len(temp_recs)):\n",
    "            try:\n",
    "                recordings.append(str(os.path.split(temp_recs[k])[1][:-4], \"utf-8\"))\n",
    "            except:\n",
    "                recordings.append(os.path.split(temp_recs[k])[1][:-4])\n",
    "\n",
    "        print (\"PROCESSING: \", name)\n",
    "\n",
    "        print (\"rec id,      rec name,           all rewarded trials,   \"+\n",
    "               str(n_sec_window) + \" sec lockout rewarded trials (*** good sessions; ####### best 3 sessions\")\n",
    "        for ctr,recording in enumerate(recordings):\n",
    "\n",
    "            # MAKE PRINTOUT TABLE\n",
    "            \n",
    "            if save_best_flag==False:\n",
    "                prefix = '       '\n",
    "                if ctr in selected_sessions_animal:\n",
    "                    if ctr in best_sessions_animal:\n",
    "                        prefix=\"#######\"\n",
    "                    else:\n",
    "                        prefix='    ***'\n",
    "                        \n",
    "            else:\n",
    "                if ctr in selected_sessions_animal:\n",
    "                    if ctr in best_sessions_animal:\n",
    "                        prefix=\"#######\"\n",
    "                    else:\n",
    "                        prefix='    ***'\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            try: \n",
    "                (data_04, data_04_random, data_04_lockout, data_04_lockout_random) = load_trial_courses_ROI_code04_trigger(\n",
    "                                                                                                      recording,\n",
    "                                                                                                      root_dir,\n",
    "                                                                                                      feature_name,\n",
    "                                                                                                      lockout_window,\n",
    "                                                                                                      n_sec_window)\n",
    "                data_04_list.append(data_04)\n",
    "                data_04_random_list.append(data_04_random)\n",
    "                data_04_lockout_list.append(data_04_lockout)\n",
    "                data_04_lockout_random_list.append(data_04_lockout_random)\n",
    "\n",
    "                session_list.append(recording)\n",
    "                ctr_list.append(ctr)\n",
    "\n",
    "            except:\n",
    "                data_04 = np.zeros((0),'float32')\n",
    "                data_04_random = data_04\n",
    "                data_04_lockout = data_04\n",
    "                data_04_lockout_random = data_04\n",
    "\n",
    "                data_04_list.append(data_04)\n",
    "                data_04_random_list.append(data_04)\n",
    "                data_04_lockout_list.append(data_04)\n",
    "                data_04_lockout_random_list.append(data_04)\n",
    "\n",
    "                session_list.append(recording)\n",
    "                ctr_list.append(ctr)\n",
    "\n",
    "\n",
    "\n",
    "            print (prefix,ctr, \"     \", recording,\"    \", data_04.shape, \"        \", data_04_lockout.shape)\n",
    "\n",
    "        np.savez(fname_out,\n",
    "                 data_04 = data_04_list, \n",
    "                 data_04_random = data_04_random_list, \n",
    "                 data_04_lockout = data_04_lockout_list,\n",
    "                 data_04_lockout_random= data_04_lockout_random_list,\n",
    "                 session_list = session_list, \n",
    "                 ctr_list = ctr_list,\n",
    "                 selected_sessions = selected_sessions_animal,\n",
    "                 best_sessions = best_sessions_animal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING:  AQ2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/110 [00:00<?, ?it/s]<ipython-input-2-41443091afec>:16: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  idx = np.where(codes==code)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_stm is None AQ2am_Dec9_30Hz\n",
      "data_stm is None AQ2am_Dec9_30Hz\n",
      "data_stm is None AQ2am_Dec10_30Hz\n",
      "data_stm is None AQ2am_Dec10_30Hz\n",
      "data_stm is None AQ2pm_Dec10_30Hz\n",
      "data_stm is None AQ2pm_Dec10_30Hz\n",
      "data_stm is None AQ2am_Dec14_30Hz\n",
      "data_stm is None AQ2am_Dec14_30Hz\n",
      "Imaging rates between aligned and session are incorrect, exiting:  0.3096286204551007\n",
      "data_stm is None AQ2am_Dec30_30Hz\n",
      "Imaging rates between aligned and session are incorrect, exiting:  0.3096286204551007\n",
      "data_stm is None AQ2am_Dec30_30Hz\n",
      "data_stm is None AQ2am_Dec22_30Hz\n",
      "data_stm is None AQ2am_Dec22_30Hz\n",
      "data_stm is None AQ2am_Dec23_30Hz\n",
      "data_stm is None AQ2am_Dec23_30Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 14/110 [03:04<21:01, 13.14s/it]<ipython-input-2-41443091afec>:16: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  idx = np.where(codes==code)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_stm is None AQ2am_Jan11_30Hz\n",
      "data_stm is None AQ2am_Jan11_30Hz\n",
      "data_stm is None AQ2am_Jan12_30Hz\n",
      "data_stm is None AQ2am_Jan12_30Hz\n",
      "data_stm is None AQ2pm_Jan18_30Hz\n",
      "data_stm is None AQ2pm_Jan18_30Hz\n",
      "data_stm is None AQ2am_Jan19_30Hz\n",
      "data_stm is None AQ2am_Jan19_30Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|       | 28/110 [08:20<21:49, 15.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_stm is None AQ2pm_Jan19_30Hz\n",
      "data_stm is None AQ2pm_Jan19_30Hz\n",
      "data_stm is None AQ2pm_Jan20_30Hz\n",
      "data_stm is None AQ2pm_Jan20_30Hz\n",
      "data_stm is None AQ2am_Jan25_30Hz\n",
      "data_stm is None AQ2am_Jan25_30Hz\n",
      "data_stm is None AQ2pm_Jan25_30Hz\n",
      "data_stm is None AQ2pm_Jan21_30Hz\n",
      "data_stm is None AQ2pm_Jan25_30Hz\n",
      "data_stm is None AQ2pm_Jan21_30Hz\n",
      "data_stm is None AQ2pm_Jan26_30Hz\n",
      "data_stm is None AQ2am_Jan21_30Hz\n",
      "data_stm is None AQ2pm_Jan26_30Hz\n",
      "data_stm is None AQ2am_Jan21_30Hz\n",
      "data_stm is None AQ2am_Jan26_30Hz\n",
      "data_stm is None AQ2am_Jan26_30Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|     | 56/110 [17:08<16:50, 18.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero length recording exiting (excitation light failure) AQ2am_Feb25_30Hz\n",
      "data_stm is None AQ2am_Feb25_30Hz\n",
      "zero length recording exiting (excitation light failure) AQ2am_Feb25_30Hz\n",
      "data_stm is None AQ2am_Feb25_30Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [53:23, 28.60s/it]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE WITH  AQ2\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "##### COMPUTE ROI TRIAL COURSES - WITH & WITHOUT LOCKOUT #########\n",
    "##################################################################\n",
    "'''  Compute calcium activity in ROIs selected (35) for \n",
    "     lever pull actiivty\n",
    "'''\n",
    "\n",
    "# select animal names\n",
    "names = ['IA1','IA2','IA3','IJ1','IJ2','AR4','AQ2']\n",
    "names = ['AQ2']\n",
    "#names = ['IA1']\n",
    "sessions = ['Feb4']\n",
    "\n",
    "from tqdm import tqdm\n",
    "feature_name = 'code_04'\n",
    "midline_filter_flag = True\n",
    "save_stm_flag = False         # flag to save raw stm maps [128 x 128 x n_times] files during processing\n",
    "transform_data_flag = False   # flag which reverts to using manually aligned/transformed data\n",
    "\n",
    "use_fixed_filter_flag = False  # use filter from a single recording on all data\n",
    "fname_filter = '/media/cat/4TBSSD/yuki/IA1/tif_files/IA1am_Mar11_30Hz/IA1am_Mar11_30Hz_code_04_trial_ROItimeCourses_15sec_midline_filter.npy'\n",
    "\n",
    "\n",
    "# window to compute\n",
    "n_sec_window = 10\n",
    "lockout_window = 10\n",
    "\n",
    "recompute = False  # overwrite previously generated data\n",
    "\n",
    "for name in names:\n",
    "    root_dir = '/media/cat/4TBSSD/yuki/'+name\n",
    "    #recordings = np.loadtxt(root_dir + '/'+name+'.txt',dtype='str')\n",
    "    \n",
    "    temp_recs = np.load(root_dir+'/tif_files.npy')\n",
    "    recordings =[]\n",
    "    for k in range(len(temp_recs)):\n",
    "        try:\n",
    "            recordings.append(str(os.path.split(temp_recs[k])[1][:-4], \"utf-8\"))\n",
    "        except:\n",
    "            recordings.append(os.path.split(temp_recs[k])[1][:-4])\n",
    "    \n",
    "    print (\"PROCESSING: \", name)\n",
    "    \n",
    "    if True:\n",
    "        import parmap\n",
    "        res = parmap.map(compute_trial_courses_ROI_code04_trigger, \n",
    "                       recordings,\n",
    "                       root_dir,\n",
    "                       feature_name,\n",
    "                       lockout_window,\n",
    "                       n_sec_window,\n",
    "                       recompute,\n",
    "                       midline_filter_flag,\n",
    "                       save_stm_flag,\n",
    "                       transform_data_flag,\n",
    "                       use_fixed_filter_flag,\n",
    "                       fname_filter,\n",
    "                       pm_processes=2,\n",
    "                       pm_pbar=True)\n",
    "    else:\n",
    "        res = []\n",
    "        for recording in tqdm(recordings):\n",
    "            print (\"recording: \", recording)\n",
    "            \n",
    "            for session in sessions:\n",
    "                #if session in recording:\n",
    "                    res.append(compute_trial_courses_ROI_code04_trigger(recording,\n",
    "                                                                root_dir,\n",
    "                                                                feature_name,\n",
    "                                                                lockout_window,\n",
    "                                                                n_sec_window,\n",
    "                                                                recompute,\n",
    "                                                                midline_filter_flag,\n",
    "                                                                save_stm_flag,\n",
    "                                                                transform_data_flag,\n",
    "                                                                use_fixed_filter_flag,\n",
    "                                                                fname_filter\n",
    "                                                                   ))\n",
    "            \n",
    "    print (\"DONE WITH \", name)\n",
    "    print ('')\n",
    "    print ('')\n",
    "    print ('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING:  IA2\n",
      "rec id,      rec name,           all rewarded trials,   10 sec lockout rewarded trials (*** good sessions; ####### best 3 sessions\n",
      "####### 3       IA2pm_Feb4_30Hz      (91, 35, 601)          (61, 35, 601)\n",
      "    *** 4       IA2pm_Feb5_30Hz      (69, 35, 601)          (49, 35, 601)\n",
      "    *** 10       IA2pm_Feb16_30Hz      (50, 35, 601)          (46, 35, 601)\n",
      "    *** 13       IA2pm_Feb19_30Hz      (68, 35, 601)          (48, 35, 601)\n",
      "    *** 14       IA2pm_Feb22_30Hz      (42, 35, 601)          (35, 35, 601)\n",
      "    *** 19       IA2pm_Feb29_30Hz      (60, 35, 601)          (48, 35, 601)\n",
      "    *** 21       IA2pm_Mar2_30Hz      (63, 35, 601)          (40, 35, 601)\n",
      "    *** 24       IA2am_Mar7_30Hz      (58, 35, 601)          (46, 35, 601)\n",
      "    *** 28       IA2am_Mar11_30Hz      (83, 35, 601)          (58, 35, 601)\n",
      "####### 29       IA2pm_Mar14_30Hz      (90, 35, 601)          (47, 35, 601)\n",
      "    *** 30       IA2am_Mar15_30Hz      (64, 35, 601)          (41, 35, 601)\n",
      "    *** 31       IA2pm_Mar16_30Hz      (100, 35, 601)          (66, 35, 601)\n",
      "    *** 32       IA2pm_Mar17_30Hz      (85, 35, 601)          (48, 35, 601)\n",
      "    *** 33       IA2pm_Mar18_30Hz      (62, 35, 601)          (35, 35, 601)\n",
      "    *** 34       IA2pm_Mar21_30Hz      (50, 35, 601)          (35, 35, 601)\n",
      "    *** 35       IA2pm_Mar23_30Hz      (67, 35, 601)          (35, 35, 601)\n",
      "    *** 39       IA2pm_Mar31_30Hz      (63, 35, 601)          (51, 35, 601)\n",
      "####### 40       IA2pm_Apr1_30Hz      (87, 35, 601)          (48, 35, 601)\n",
      "    *** 43       IA2pm_Apr6_30Hz      (64, 35, 601)          (41, 35, 601)\n"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "#### MAKE TIME ORDERED .NZP FILES ##################\n",
    "#########################################################\n",
    "\n",
    "# MANUALLY FIND WHICH SESSIONS HAVE LARGE DIFFERENCE in BEWEEN WITH /WITHOUT PULL OVERLAP IN 10SEC WINDOW\n",
    "# AQ2\n",
    "selected_sessions_AQ2 = [32,50,53,54,56,57,58,59,63,64,65,66,67,70,71,72,73,74,77,78,79,80,81]\n",
    "selected_sessions_AQ2 = np.append(np.append(selected_sessions_AQ2,np.arange(83,106)),np.arange(108,110,1))\n",
    "best_sessions_AQ2 = [32,58,91]\n",
    "\n",
    "# IA1\n",
    "selected_sessions_IA1 = [2,5,7,10,13,15,29,52,54,65]\n",
    "best_sessions_IA1 = [13,15,29]\n",
    "\n",
    "# IA2\n",
    "selected_sessions_IA2 = [3,4,10,13,14,19,21,24,28,29,30,31,32,33,34,35,39,40,43]\n",
    "best_sessions_IA2 = [3,29,40]\n",
    "\n",
    "# IA3\n",
    "selected_sessions_IA3 = [1,2,7,9,10,17,18,21,24,25,30,31,32,33,34,35,37,38,39,40,41,42,43]\n",
    "best_sessions_IA3 = [2,34,43]\n",
    "\n",
    "# IJ1\n",
    "selected_sessions_IJ1 = [3,4,5,6,7,8,10,14,18,20,21,22,28,31,37,38,41]\n",
    "best_sessions_IJ1 = [14,22,38]\n",
    "\n",
    "# IJ2\n",
    "selected_sessions_IJ2 = [2,3,4,6,7,10,17,18,19,20,21,24,26,29,33,37,39,40,41,43,46]\n",
    "best_sessions_IJ2 = [20,33,40]\n",
    "\n",
    "# AR4\n",
    "selected_sessions_AR4 = [6,12,14,23,26,27,28,30,31,32]\n",
    "best_sessions_AR4 = [6,23,30]\n",
    "\n",
    "\n",
    "####################################################\n",
    "# SELECT A RECORDING TO COMBINE\n",
    "# CHECK ALL VS. LOCKOUT DATA\n",
    "#names = ['IA1','IA2','IA3','IJ1','IJ2','AR4','AQ2']\n",
    "names = ['IA2']\n",
    "\n",
    "selected_sessions_animal = selected_sessions_IA2\n",
    "best_sessions_animal = best_sessions_IA2\n",
    "\n",
    "# save_npz_data(names, \n",
    "#               lockout_window,\n",
    "#               n_sec_window,\n",
    "#               feature_name,\n",
    "#               selected_sessions_animal, \n",
    "#               best_sessions_animal)\n",
    "\n",
    "save_dir = '/media/cat/4TBSSD/yuki/'\n",
    "save_best_flag = True\n",
    "save_npz_data(save_dir,\n",
    "                  names, \n",
    "                  lockout_window,\n",
    "                  n_sec_window,\n",
    "                  feature_name,\n",
    "                  selected_sessions_animal, \n",
    "                  best_sessions_animal,\n",
    "                  save_best_flag)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "########### SAVE MULTI SESSION DATA ###########\n",
    "###############################################\n",
    "\n",
    "########## RANDOM DATA **********\n",
    "fnames = [\n",
    "'/media/cat/4TBSSD/yuki/IA1/tif_files/IA1pm_Feb23_30Hz/IA1pm_Feb23_30Hz_code_04_lockout_8sec_random_ROItimeCourses_10sec.npy'\n",
    "]\n",
    "\n",
    "lockout_flag = True\n",
    "\n",
    "temp = []\n",
    "for fname in fnames:\n",
    "    if lockout_flag:\n",
    "        data = np.load(fname)\n",
    "    else:\n",
    "        data = np.load(fname.replace('lockout_8sec_',''))\n",
    "    print (data.shape)\n",
    "    temp.extend(data)\n",
    "    \n",
    "temp = np.array(temp)[:52]\n",
    "print (temp.shape)\n",
    "\n",
    "if lockout_flag:\n",
    "    np.save('/home/cat/lockout_random.npy', temp)\n",
    "else:\n",
    "    np.save('/home/cat/all_random.npy', temp)\n",
    "print ('')   \n",
    "    \n",
    "\n",
    "########## TRIAL DATA **********\n",
    "fnames = [\n",
    "'/media/cat/4TBSSD/yuki/IA1/tif_files/IA1pm_Feb23_30Hz/IA1pm_Feb23_30Hz_code_04_lockout_8sec_trial_ROItimeCourses_10sec.npy'\n",
    "]\n",
    "\n",
    "temp = []\n",
    "for fname in fnames:\n",
    "    if lockout_flag:\n",
    "        data = np.load(fname)\n",
    "    else:\n",
    "        data = np.load(fname.replace('lockout_8sec_',''))\n",
    "    print (data.shape)\n",
    "    temp.extend(data)\n",
    "    \n",
    "temp = np.array(temp)[:52]\n",
    "print (temp.shape)\n",
    "\n",
    "if lockout_flag:\n",
    "    np.save('/home/cat/lockout_trials.npy', temp)\n",
    "else:\n",
    "    np.save('/home/cat/all_trials.npy', temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING:  AQ2\n",
      "rec id,      rec name,           all rewarded trials,   10sec lockout rewarded trials\n",
      "*** 3       AQ2am_Dec11_30Hz      (7, 35, 601)          (7, 35, 601)\n",
      "    4       AQ2pm_Dec14_30Hz      (10, 35, 601)          (9, 35, 601)\n",
      "    6       AQ2pm_Dec16_30Hz      (27, 35, 601)          (22, 35, 601)\n",
      "    7       AQ2am_Dec17_30Hz      (21, 35, 601)          (15, 35, 601)\n",
      "    8       AQ2pm_Dec17_30Hz      (5, 35, 601)          (5, 35, 601)\n",
      "    9       AQ2am_Dec18_30Hz      (7, 35, 601)          (7, 35, 601)\n",
      "    10       AQ2pm_Dec18_30Hz      (5, 35, 601)          (5, 35, 601)\n",
      "    11       AQ2am_Dec21_30Hz      (2, 35, 601)          (2, 35, 601)\n",
      "    14       AQ2am_Dec28_30Hz      (24, 35, 601)          (13, 35, 601)\n",
      "    15       AQ2am_Dec29_30Hz      (33, 35, 601)          (27, 35, 601)\n",
      "    17       AQ2am_Dec31_30Hz      (65, 35, 601)          (51, 35, 601)\n",
      "    18       AQ2am_Jan4_30Hz      (52, 35, 601)          (42, 35, 601)\n",
      "    19       AQ2am_Jan5_30Hz      (52, 35, 601)          (39, 35, 601)\n",
      "    20       AQ2am_Jan6_30Hz      (59, 35, 601)          (49, 35, 601)\n",
      "    21       AQ2am_Jan7_30Hz      (4, 35, 601)          (4, 35, 601)\n",
      "    22       AQ2am_Jan8_30Hz      (27, 35, 601)          (22, 35, 601)\n",
      "    23       AQ2pm_Jan11_30Hz      (17, 35, 601)          (16, 35, 601)\n",
      "    25       AQ2pm_Jan12_30Hz      (2, 35, 601)          (2, 35, 601)\n",
      "    27       AQ2pm_Jan13_30Hz      (37, 35, 601)          (31, 35, 601)\n",
      "    28       AQ2am_Jan13_30Hz      (55, 35, 601)          (43, 35, 601)\n",
      "    29       AQ2pm_Jan14_30Hz      (37, 35, 601)          (34, 35, 601)\n",
      "*** 30       AQ2am_Jan14_30Hz      (43, 35, 601)          (37, 35, 601)\n",
      "    31       AQ2am_Jan15_30Hz      (26, 35, 601)          (23, 35, 601)\n",
      "    32       AQ2pm_Jan15_30Hz      (70, 35, 601)          (41, 35, 601)\n",
      "    34       AQ2am_Jan18_30Hz      (13, 35, 601)          (10, 35, 601)\n",
      "    38       AQ2am_Jan20_30Hz      (2, 35, 601)          (2, 35, 601)\n",
      "    41       AQ2pm_Jan22_30Hz      (3, 35, 601)          (3, 35, 601)\n",
      "    42       AQ2am_Jan22_30Hz      (4, 35, 601)          (4, 35, 601)\n",
      "    47       AQ2am_Jan27_30Hz      (3, 35, 601)          (3, 35, 601)\n",
      "    48       AQ2pm_Jan27_30Hz      (55, 35, 601)          (35, 35, 601)\n",
      "    49       AQ2am_Jan28_30Hz      (46, 35, 601)          (43, 35, 601)\n",
      "*** 50       AQ2pm_Jan28_30Hz      (61, 35, 601)          (47, 35, 601)\n",
      "    51       AQ2am_Jan29_30Hz      (41, 35, 601)          (34, 35, 601)\n",
      "    52       AQ2pm_Jan29_30Hz      (50, 35, 601)          (27, 35, 601)\n",
      "    53       AQ2am_Feb2_30Hz      (68, 35, 601)          (46, 35, 601)\n",
      "    54       AQ2am_Feb3_30Hz      (79, 35, 601)          (52, 35, 601)\n",
      "    55       AQ2am_Feb4_30Hz      (21, 35, 601)          (21, 35, 601)\n",
      "    56       AQ2am_Feb5_30Hz      (74, 35, 601)          (52, 35, 601)\n",
      "    57       AQ2am_Feb9_30Hz      (91, 35, 601)          (47, 35, 601)\n",
      "    58       AQ2am_Feb10_30Hz      (93, 35, 601)          (54, 35, 601)\n",
      "    59       AQ2am_Feb11_30Hz      (131, 35, 601)          (44, 35, 601)\n",
      "    60       AQ2am_Feb12_30Hz      (184, 35, 601)          (19, 35, 601)\n",
      "    61       AQ2am_Feb15_30Hz      (158, 35, 601)          (19, 35, 601)\n",
      "    62       AQ2am_Feb16_30Hz      (145, 35, 601)          (32, 35, 601)\n",
      "    63       AQ2am_Feb17_30Hz      (104, 35, 601)          (54, 35, 601)\n",
      "    64       AQ2am_Feb18_30Hz      (156, 35, 601)          (35, 35, 601)\n",
      "    65       AQ2am_Feb19_30Hz      (96, 35, 601)          (41, 35, 601)\n",
      "    66       AQ2am_Feb22_30Hz      (145, 35, 601)          (39, 35, 601)\n",
      "    67       AQ2am_Feb23_30Hz      (121, 35, 601)          (51, 35, 601)\n",
      "    69       AQ2am_Feb26_30Hz      (92, 35, 601)          (29, 35, 601)\n",
      "    70       AQ2am_Feb29_30Hz      (91, 35, 601)          (44, 35, 601)\n",
      "    71       AQ2am_Mar1_30Hz      (127, 35, 601)          (37, 35, 601)\n",
      "    72       AQ2am_Mar2_30Hz      (58, 35, 601)          (36, 35, 601)\n",
      "    73       AQ2am_Mar3_30Hz      (131, 35, 601)          (38, 35, 601)\n",
      "    74       AQ2pm_Mar7_Day3_30Hz      (110, 35, 601)          (35, 35, 601)\n",
      "    75       AQ2pm_Mar9_Day5_30Hz      (71, 35, 601)          (27, 35, 601)\n",
      "    76       AQ2pm_Mar11_Day7_30Hz      (99, 35, 601)          (28, 35, 601)\n",
      "    77       AQ2am_Mar14_Week2_30Hz      (104, 35, 601)          (42, 35, 601)\n",
      "    78       AQ2pm_Mar15_Week2_30Hz      (144, 35, 601)          (36, 35, 601)\n",
      "    79       AQ2am_Mar16_Week2_30Hz      (122, 35, 601)          (37, 35, 601)\n",
      "    80       AQ2am_Mar17_Week2_30Hz      (104, 35, 601)          (50, 35, 601)\n",
      "    81       AQ2am_Mar18_Week2_30Hz      (140, 35, 601)          (45, 35, 601)\n",
      "    82       AQ2am_Mar21_Week3_30Hz      (48, 35, 601)          (32, 35, 601)\n",
      "    83       AQ2am_Mar22_Week3_30Hz      (64, 35, 601)          (47, 35, 601)\n",
      "    84       AQ2am_Mar23_Week3_30Hz      (85, 35, 601)          (42, 35, 601)\n",
      "    85       AQ2am_Mar24_Week3_30Hz      (125, 35, 601)          (44, 35, 601)\n",
      "    86       AQ2am_Mar29_Week4_30Hz      (91, 35, 601)          (47, 35, 601)\n",
      "    87       AQ2am_Mar30_Week4_30Hz      (149, 35, 601)          (43, 35, 601)\n",
      "    88       AQ2am_Mar31_Week4_30Hz      (116, 35, 601)          (45, 35, 601)\n",
      "    89       AQ2am_Apr1_Week4_30Hz      (138, 35, 601)          (44, 35, 601)\n",
      "    90       AQ2am_Apr4_Week5_30Hz      (127, 35, 601)          (48, 35, 601)\n",
      "    91       AQ2am_Apr5_Week5_30Hz      (100, 35, 601)          (56, 35, 601)\n",
      "    92       AQ2am_Apr6_Week5_30Hz      (118, 35, 601)          (44, 35, 601)\n",
      "    93       AQ2am_Apr7_Week5_30Hz      (93, 35, 601)          (49, 35, 601)\n",
      "    94       AQ2am_Apr8_Week5_30Hz      (105, 35, 601)          (46, 35, 601)\n",
      "    95       AQ2am_Apr11_Week6_30Hz      (144, 35, 601)          (51, 35, 601)\n",
      "    96       AQ2am_Apr12_Week6_30Hz      (94, 35, 601)          (52, 35, 601)\n",
      "    97       AQ2am_Apr13_Week6_30Hz      (106, 35, 601)          (57, 35, 601)\n",
      "    98       AQ2am_Apr14_Week6_30Hz      (99, 35, 601)          (60, 35, 601)\n",
      "    99       AQ2am_Apr15_Week6_30Hz      (84, 35, 601)          (53, 35, 601)\n",
      "    100       AQ2am_Apr18_Week7_30Hz      (77, 35, 601)          (53, 35, 601)\n",
      "    101       AQ2am_Apr19_Week7_30Hz      (71, 35, 601)          (49, 35, 601)\n",
      "    102       AQ2am_Apr20_Week7_30Hz      (69, 35, 601)          (45, 35, 601)\n",
      "    103       AQ2am_Apr21_Week7_30Hz      (53, 35, 601)          (37, 35, 601)\n",
      "    104       AQ2am_Apr22_Week7_30Hz      (123, 35, 601)          (47, 35, 601)\n",
      "    105       AQ2am_Apr25_Week8_30Hz      (114, 35, 601)          (36, 35, 601)\n",
      "    106       AQ2am_Apr26_Week8_30Hz      (112, 35, 601)          (47, 35, 601)\n",
      "    107       AQ2am_Apr27_Week8_30Hz      (138, 35, 601)          (28, 35, 601)\n",
      "    108       AQ2am_Apr28_Week8_30Hz      (86, 35, 601)          (53, 35, 601)\n",
      "    109       AQ2am_Apr29_Week8_30Hz      (85, 35, 601)          (51, 35, 601)\n"
     ]
    }
   ],
   "source": [
    "# # CHECK ALL VS. LOCKOUT DATA\n",
    "# names = ['IA1','IA2','IA3','IJ1','IJ2','AR4','AQ2']\n",
    "# names = ['AQ2']\n",
    "\n",
    "# selected_sessions = [3, 30, 50]\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# feature_name = 'code_04'\n",
    "# lockout_window = 10\n",
    "# n_sec_window = 10\n",
    "\n",
    "# for name in names:\n",
    "#     root_dir = '/media/cat/4TBSSD/yuki/'+name\n",
    "#     #recordings = np.loadtxt(root_dir + '/'+name+'.txt',dtype='str')\n",
    "    \n",
    "#     temp_recs = np.load(root_dir+'/tif_files.npy')\n",
    "#     recordings =[]\n",
    "#     for k in range(len(temp_recs)):\n",
    "#         try:\n",
    "#             recordings.append(str(os.path.split(temp_recs[k])[1][:-4], \"utf-8\"))\n",
    "#         except:\n",
    "#             recordings.append(os.path.split(temp_recs[k])[1][:-4])\n",
    "    \n",
    "#     print (\"PROCESSING: \", name)\n",
    "    \n",
    "#     print (\"rec id,      rec name,           all rewarded trials,   10sec lockout rewarded trials\")\n",
    "#     for ctr,recording in enumerate(recordings):\n",
    "\n",
    "#         try: \n",
    "#             (data_04, data_04_random, data_04_lockout, data_04_lockout_random) = load_trial_courses_ROI_code04_trigger(\n",
    "#                                                                       recording,\n",
    "#                                                                       root_dir,\n",
    "#                                                                       feature_name,\n",
    "#                                                                       lockout_window,\n",
    "#                                                                       n_sec_window)\n",
    "#             if ctr not in selected_sessions:\n",
    "#                 print (\"   \",ctr, \"     \", recording,\"    \", data_04.shape, \"        \", data_04_lockout.shape)\n",
    "#             else:\n",
    "#                 print (\"***\", ctr, \"     \", recording,\"    \", data_04.shape, \"        \", data_04_lockout.shape)\n",
    "            \n",
    "#             #print ('')\n",
    "#         except:\n",
    "#             pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MAKE ORDERED .NZP FILES\n",
    "# # MUST INPUT a temporally ordered.txt file\n",
    "# fname_list = '/media/cat/4TBSSD/yuki/time_courses/AQ2_ordered.txt'\n",
    "# fnames = np.loadtxt(fname_list,dtype='str')\n",
    "# #print (fnames)\n",
    "\n",
    "# data_list = []\n",
    "# fnames_data_list = []\n",
    "# data_random_list = []\n",
    "# fnames_data_random_list = []\n",
    "# for fname in fnames:\n",
    "#     if '04' in fname:\n",
    "#         data_list.append(np.load(fname))\n",
    "#         fnames_data_list.append(fname)\n",
    "#     else:\n",
    "#         data_random_list.append(np.load(fname))\n",
    "#         fnames_data_random_list.append(fname)\n",
    "\n",
    "# np.savez(fname_list[:-4]+'.npz',\n",
    "#         data_04_lever_pull = data_list,\n",
    "#         data_04_lever_pull_fnames = fnames_data_list,\n",
    "#         data_random = data_random_list,\n",
    "#         data_random_fnames = fnames_data_random_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET DISTRIBUTION OF SESSIONS IN EACH ANIMAL\n",
    "\n",
    "all_files = [\n",
    "'/media/cat/4TBSSD/yuki/time_courses/AR4_ordered.npz',\n",
    "'/media/cat/4TBSSD/yuki/time_courses/IA1_ordered.npz',\n",
    "'/media/cat/4TBSSD/yuki/time_courses/IA2_ordered.npz',\n",
    "'/media/cat/4TBSSD/yuki/time_courses/IA3_ordered.npz',\n",
    "'/media/cat/4TBSSD/yuki/time_courses/IJ1_ordered.npz',\n",
    "'/media/cat/4TBSSD/yuki/time_courses/IJ2_ordered.npz',\n",
    "'/media/cat/4TBSSD/yuki/time_courses/AQ2_ordered.npz'\n",
    "]\n",
    "\n",
    "tot = 0\n",
    "for ctr, file_ in enumerate(all_files):\n",
    "    temp = np.load(file_, allow_pickle=True)\n",
    "    #fnames = temp['data_04_lever_pull_fnames']\n",
    "    #print (fnames)\n",
    "    data = temp['data_04_lever_pull']\n",
    "    print (file_, \" # of sessions: \", len(data))\n",
    "    tot+=len(data)\n",
    "    ax=plt.subplot(2,4,ctr+1)\n",
    "    plt.title(os.path.split(file_)[1])\n",
    "    \n",
    "    lengths = []\n",
    "    for k in range(len(data)):\n",
    "        lengths.append(len(data[k]))\n",
    "        #print (data[k].shape)\n",
    "    y = np.histogram(lengths, np.arange(0,400,10))\n",
    "    plt.plot(y[1][:-1],y[0])\n",
    "    plt.ylabel(\"# of sessions\")\n",
    "    plt.xlabel(\"# of trials in sessions\")\n",
    "    plt.xlim(0,200)\n",
    "print (\"total: \", tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE DATA\n",
    "#np.save(root_dir+\"data_random_code_\"+str(locs_selected.shape[0])+\"trials_.npy\", data_stm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 35, 601)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/media/cat/4TBSSD/yuki/IA2/tif_files/IA2pm_Apr6_30Hz/data_random_code_64trials_ROIs.npy')\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Oct27pm_15Hz_8x8/AR4_Oct27pm_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Oct28pm_15Hz_8x8/AR4_Oct28pm_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Oct29pm_15Hz_8x8/AR4_Oct29pm_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov3pm_15Hz_8x8/AR4_Nov3pm_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov3am_15Hz_8x8/AR4_Nov3am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov4am_15Hz_8x8/AR4_Nov4am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov5am_15Hz_8x8/AR4_Nov5am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov6am_15Hz_8x8/AR4_Nov6am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov7am_15Hz_8x8/AR4_Nov7am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov10am_15Hz_8x8/AR4_Nov10am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov11am_15Hz_8x8/AR4_Nov11am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov12am_15Hz_8x8/AR4_Nov12am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov17am_15Hz_8x8/AR4_Nov17am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov19am_15Hz_8x8/AR4_Nov19am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov21am_15Hz_8x8/AR4_Nov21am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov22am_15Hz_8x8/AR4_Nov22am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov23am_15Hz_8x8/AR4_Nov23am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov24am_15Hz_8x8/AR4_Nov24am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov25am_15Hz_8x8/AR4_Nov25am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov26am_15Hz_8x8/AR4_Nov26am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov27am_15Hz_8x8/AR4_Nov27am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Nov30am_15Hz_8x8/AR4_Nov30am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Dec1am_15Hz_8x8/AR4_Dec1am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Dec2am_15Hz_8x8/AR4_Dec2am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Dec3am_15Hz_8x8/AR4_Dec3am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Dec8am_15Hz_8x8/AR4_Dec8am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Dec9am_15Hz_8x8/AR4_Dec9am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Dec10am_15Hz_8x8/AR4_Dec10am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Dec11am_15Hz_8x8/AR4_Dec11am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Dec12am_15Hz_8x8/AR4_Dec12am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Dec15am_15Hz_8x8/AR4_Dec15am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Dec16am_15Hz_8x8/AR4_Dec16am_15Hz_8x8.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/AR4/tif_files/AR4_Dec17am_15Hz_8x8/AR4_Dec17am_15Hz_8x8.tif']\n"
     ]
    }
   ],
   "source": [
    "data= np.load('/media/cat/4TBSSD/yuki/AR4/tif_files.npy')\n",
    "print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb1_30Hz/IA1pm_Feb1_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb2_30Hz/IA1pm_Feb2_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb3_30Hz/IA1pm_Feb3_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb4_30Hz/IA1pm_Feb4_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb5_30Hz/IA1pm_Feb5_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb9_30Hz/IA1pm_Feb9_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb10_30Hz/IA1pm_Feb10_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb11_30Hz/IA1pm_Feb11_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb12_30Hz/IA1pm_Feb12_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb15_30Hz/IA1pm_Feb15_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb16_30Hz/IA1pm_Feb16_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb17_30Hz/IA1pm_Feb17_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb18_30Hz/IA1pm_Feb18_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb19_30Hz/IA1pm_Feb19_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb22_30Hz/IA1pm_Feb22_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb23_30Hz/IA1pm_Feb23_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb24_30Hz/IA1pm_Feb24_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb25_30Hz/IA1pm_Feb25_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb26_30Hz/IA1pm_Feb26_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Feb29_30Hz/IA1pm_Feb29_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Mar1_30Hz/IA1pm_Mar1_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Mar2_30Hz/IA1pm_Mar2_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Mar3_30Hz/IA1pm_Mar3_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_Mar4_30Hz/IA1am_Mar4_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_Mar7_30Hz/IA1am_Mar7_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Mar8_30Hz/IA1pm_Mar8_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_Mar9_30Hz/IA1am_Mar9_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_Mar10_30Hz/IA1am_Mar10_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_Mar11_30Hz/IA1am_Mar11_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Mar14_30Hz/IA1pm_Mar14_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_Mar15_30Hz/IA1am_Mar15_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Mar16_30Hz/IA1pm_Mar16_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Mar17_30Hz/IA1pm_Mar17_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Mar18_30Hz/IA1pm_Mar18_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Mar21_30Hz/IA1pm_Mar21_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Mar22_30Hz/IA1pm_Mar22_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Mar23_30Hz/IA1pm_Mar23_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Mar24_30Hz/IA1pm_Mar24_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Mar29_30Hz/IA1pm_Mar29_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Mar30_30Hz/IA1pm_Mar30_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Mar31_30Hz/IA1pm_Mar31_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr1_30Hz/IA1pm_Apr1_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr4_30Hz/IA1pm_Apr4_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr5_30Hz/IA1pm_Apr5_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr6_30Hz/IA1pm_Apr6_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr7_30Hz/IA1pm_Apr7_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr11_Day3_30Hz/IA1pm_Apr11_Day3_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr13_Day5_30Hz/IA1pm_Apr13_Day5_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr15_Day7_30Hz/IA1pm_Apr15_Day7_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr18_Week2_30Hz/IA1pm_Apr18_Week2_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr19_Week2_30Hz/IA1pm_Apr19_Week2_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr20_Week2_30Hz/IA1pm_Apr20_Week2_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr21_Week2_30Hz/IA1pm_Apr21_Week2_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr22_Week2_30Hz/IA1pm_Apr22_Week2_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr25_Week3_30Hz/IA1pm_Apr25_Week3_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr26_Week3_30Hz/IA1pm_Apr26_Week3_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr27_Week3_30Hz/IA1pm_Apr27_Week3_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr28_Week3_30Hz/IA1pm_Apr28_Week3_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1pm_Apr29_Week3_30Hz/IA1pm_Apr29_Week3_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_May2_Week4_30Hz/IA1am_May2_Week4_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_May4_Week4_30Hz/IA1am_May4_Week4_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_May5_Week4_30Hz/IA1am_May5_Week4_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_May6_Week4_30Hz/IA1am_May6_Week4_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_May9_Week5_30Hz/IA1am_May9_Week5_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_May10_Week5_30Hz/IA1am_May10_Week5_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_May12_Week5_30Hz/IA1am_May12_Week5_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_May13_Week5_30Hz/IA1am_May13_Week5_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_May16_Week6_30Hz/IA1am_May16_Week6_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_May17_Week6_30Hz/IA1am_May17_Week6_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_May18_Week6_30Hz/IA1am_May18_Week6_30Hz.tif'\n",
      " '/media/cat/12TB/in_vivo/tim/yuki/IA1/tif_files/IA1am_May20_Week6_30Hz/IA1am_May20_Week6_30Hz.tif']\n"
     ]
    }
   ],
   "source": [
    "print (np.load('/media/cat/4TBSSD/yuki/IA1/tif_files.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['df_with_missing']>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = \"/home/cat/Downloads/AR4_2014-12-02_12-56-34.975DLC_resnet50_yuki_leverJul21shuffle1_200000.h5\"\n",
    "\n",
    "f = h5py.File(filename, \"r\")\n",
    "# List all groups\n",
    "print(\"Keys: %s\" % f.keys())\n",
    "a_group_key = list(f.keys())[0]\n",
    "\n",
    "# Get the data\n",
    "data = list(f[a_group_key])\n",
    "\n",
    "data2 = f['df_with_missing']['table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21,)\n"
     ]
    }
   ],
   "source": [
    "print (data2[0][1].shape)\n",
    "\n",
    "traces = []\n",
    "for k in range(len(data2)):\n",
    "    traces.append(data2[k][1].reshape(-1,3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20066, 7, 3)\n"
     ]
    }
   ],
   "source": [
    "traces = np.array(traces)\n",
    "print (traces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2be8bd57af7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/media/cat/4TBSSD/yuki/IJ1/tif_files.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "files = np.load('/media/cat/4TBSSD/yuki/IJ1/tif_files.npy')\n",
    "print (files[0].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
